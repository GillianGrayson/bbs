{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Debugging autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T14:29:22.801727Z",
     "start_time": "2024-07-01T14:29:18.695223Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pytorch_tabular.utils import load_covertype_dataset\n",
    "from rich.pretty import pprint\n",
    "from sklearn.model_selection import BaseCrossValidator, ParameterGrid, ParameterSampler\n",
    "import torch\n",
    "import pickle\n",
    "import shutil\n",
    "import shap\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from glob import glob\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pytorch_tabular.utils import make_mixed_dataset, print_metrics\n",
    "from pytorch_tabular import available_models\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig, GANDALFConfig, TabNetModelConfig, FTTransformerConfig, DANetConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from pytorch_tabular.tabular_model_tuner import TabularModelTuner\n",
    "from torchmetrics.functional.regression import mean_absolute_error, pearson_corrcoef\n",
    "from pytorch_tabular import MODEL_SWEEP_PRESETS\n",
    "import pandas as pd\n",
    "from pytorch_tabular import model_sweep\n",
    "from src.pt.model_sweep import model_sweep_custom\n",
    "import warnings\n",
    "from src.utils.configs import read_parse_config\n",
    "from src.utils.hash import dict_hash\n",
    "from src.pt.hyper_opt import train_hyper_opt\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import distinctipy\n",
    "import matplotlib.patheffects as pe\n",
    "import matplotlib.colors as mcolors\n",
    "from statannotations.Annotator import Annotator\n",
    "from scipy.stats import mannwhitneyu\n",
    "import optuna\n",
    "from scipy import stats\n",
    "from regression_bias_corrector import LinearBiasCorrector\n",
    "\n",
    "\n",
    "def make_rgb_transparent(rgb, bg_rgb, alpha):\n",
    "    return [alpha * c1 + (1 - alpha) * c2 for (c1, c2) in zip(rgb, bg_rgb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"E:/YandexDisk/Work/bbd/mriya\"\n",
    "path_configs = \"E:/Git/bbs/notebooks/mriya/configs\"\n",
    "\n",
    "feats_set = 'Все'\n",
    "models_type = 'models/oct2025'\n",
    "\n",
    "path_ckpts = f\"E:/Git/bbs/notebooks/mriya/pt/{feats_set}\"\n",
    "\n",
    "tst_n_splits = 5\n",
    "tst_n_repeats = 5\n",
    "tst_random_state = 1337\n",
    "tst_split_id = 1\n",
    "\n",
    "val_n_splits = 4\n",
    "val_n_repeats = 4\n",
    "val_random_state = 1337\n",
    "val_fold_id = 6\n",
    "\n",
    "data = pd.read_excel(f\"{path}/{models_type}/{feats_set}/data.xlsx\", index_col=0)\n",
    "df_feats = pd.read_excel(f\"{path}/{models_type}/{feats_set}/feats.xlsx\", index_col=0)\n",
    "\n",
    "feat_trgt = 'Age'\n",
    "feats_cnt = df_feats.index[df_feats['data_type'].isin(['decimal', 'integer'])].to_list()\n",
    "feats_cat = df_feats.index[df_feats['data_type'].isin(['enum'])].to_list()\n",
    "\n",
    "feats = list(feats_cnt) + list(feats_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path}/{models_type}/{feats_set}/samples_tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_val({val_random_state}_{val_n_splits}_{val_n_repeats}).pickle\", 'rb') as handle:\n",
    "    samples = pickle.load(handle)\n",
    "\n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        test_samples = samples[split_id]['test']\n",
    "        train_samples = samples[split_id]['trains'][fold_id]\n",
    "        validation_samples = samples[split_id]['validations'][fold_id]\n",
    "\n",
    "        intxns = {\n",
    "            'train_validation': set.intersection(set(train_samples), set(validation_samples)),\n",
    "            'validation_test': set.intersection(set(validation_samples), set(test_samples)),\n",
    "            'train_test': set.intersection(set(train_samples), set(test_samples))\n",
    "        }\n",
    "        \n",
    "        for intxn_name, intxn_samples in intxns.items():\n",
    "            if len(intxn_samples) > 0:\n",
    "                print(f\"Non-zero {intxn_name} intersection ({len(intxn_samples)}) for {split_id} Split and {fold_id} Fold!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validation, Test selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict = samples[tst_split_id]\n",
    "\n",
    "test = data.loc[split_dict['test'], feats + [feat_trgt]]\n",
    "train = data.loc[split_dict['trains'][val_fold_id], feats + [feat_trgt]]\n",
    "validation = data.loc[split_dict['validations'][val_fold_id], feats + [feat_trgt]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_target = 451  # 1337 42 451 1984 1899 1408\n",
    "\n",
    "models_runs = {\n",
    "    # 'GANDALF': {\n",
    "    #     'config': GANDALFConfig,\n",
    "    #     'n_trials': 512,\n",
    "    #     'seed': seed_target,\n",
    "    #     'n_startup_trials': 256,\n",
    "    #     'n_ei_candidates': 16\n",
    "    # },\n",
    "    # 'FTTransformer': {\n",
    "    #     'config': FTTransformerConfig,\n",
    "    #     'n_trials': 1024,\n",
    "    #     'seed': seed_target,\n",
    "    #     'n_startup_trials': 256,\n",
    "    #     'n_ei_candidates': 16\n",
    "    # },\n",
    "    'DANet': {\n",
    "        'config': DANetConfig,\n",
    "        'n_trials': 512,\n",
    "        'seed': seed_target,\n",
    "        'n_startup_trials': 256,\n",
    "        'n_ei_candidates': 16\n",
    "    },\n",
    "    # 'CategoryEmbeddingModel': {\n",
    "    #     'config': CategoryEmbeddingModelConfig,\n",
    "    #     'n_trials': 256,\n",
    "    #     'seed': seed_target,\n",
    "    #     'n_startup_trials': 64,\n",
    "    #     'n_ei_candidates': 16\n",
    "    # },\n",
    "    # 'TabNetModel': {\n",
    "    #     'config': TabNetModelConfig,\n",
    "    #     'n_trials': 256,\n",
    "    #     'seed': seed_target,\n",
    "    #     'n_startup_trials': 64,\n",
    "    #     'n_ei_candidates': 16\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_models = []\n",
    "\n",
    "for model_name, model_run in models_runs.items():\n",
    "\n",
    "    model_config_name = model_run['config']\n",
    "    n_trials = model_run['n_trials']\n",
    "    seed = model_run['seed']\n",
    "    n_startup_trials = model_run['n_startup_trials']\n",
    "    n_ei_candidates = model_run['n_ei_candidates']\n",
    "\n",
    "    data_config = read_parse_config(f\"{path_configs}/DataConfig.yaml\", DataConfig)\n",
    "    data_config['target'] = [feat_trgt]\n",
    "    data_config['continuous_cols'] = feats_cnt\n",
    "    data_config['categorical_cols'] = feats_cat\n",
    "    trainer_config = read_parse_config(f\"{path_configs}/TrainerConfig.yaml\", TrainerConfig)\n",
    "    pathlib.Path(path_ckpts).mkdir(parents=True, exist_ok=True)\n",
    "    trainer_config['checkpoints_path'] = path_ckpts\n",
    "    optimizer_config = read_parse_config(f\"{path_configs}/OptimizerConfig.yaml\", OptimizerConfig)\n",
    "\n",
    "    lr_find_min_lr = 1e-8\n",
    "    lr_find_max_lr = 10\n",
    "    lr_find_num_training = 256\n",
    "    lr_find_mode = \"exponential\"\n",
    "    lr_find_early_stop_threshold = 8.0\n",
    "\n",
    "    trainer_config['seed'] = seed\n",
    "    trainer_config['checkpoints'] = 'valid_loss'\n",
    "    trainer_config['load_best'] = True\n",
    "    trainer_config['auto_lr_find'] = False\n",
    "\n",
    "    model_config_default = read_parse_config(f\"{path_configs}/models/{model_name}Config.yaml\", model_config_name)\n",
    "    tabular_model_default = TabularModel(\n",
    "        data_config=data_config,\n",
    "        model_config=model_config_default,\n",
    "        optimizer_config=optimizer_config,\n",
    "        trainer_config=trainer_config,\n",
    "        verbose=False,\n",
    "    )\n",
    "    datamodule = tabular_model_default.prepare_dataloader(train=train, validation=validation, seed=seed)\n",
    "\n",
    "    opt_parts = ['test', 'validation']\n",
    "    opt_metrics = [('mean_absolute_error', 'minimize')]\n",
    "    opt_directions = []\n",
    "    for part in opt_parts:\n",
    "        for metric_pair in opt_metrics:\n",
    "            opt_directions.append(f\"{metric_pair[1]}\")\n",
    "\n",
    "    trials_results = []\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=model_name,\n",
    "        sampler=optuna.samplers.TPESampler(\n",
    "            n_startup_trials=n_startup_trials,\n",
    "            n_ei_candidates=n_ei_candidates,\n",
    "            seed=seed,\n",
    "        ),\n",
    "        directions=opt_directions\n",
    "    )\n",
    "    study.optimize(\n",
    "        func=lambda trial: train_hyper_opt(\n",
    "            trial=trial,\n",
    "            trials_results=trials_results,\n",
    "            opt_metrics=opt_metrics,\n",
    "            opt_parts=opt_parts,\n",
    "            model_config_default=model_config_default,\n",
    "            data_config_default=data_config,\n",
    "            optimizer_config_default=optimizer_config,\n",
    "            trainer_config_default=trainer_config,\n",
    "            experiment_config_default=None,\n",
    "            train=train,\n",
    "            validation=validation,\n",
    "            test=test,\n",
    "            datamodule=datamodule,\n",
    "            min_lr=lr_find_min_lr,\n",
    "            max_lr=lr_find_max_lr,\n",
    "            num_training=lr_find_num_training,\n",
    "            mode=lr_find_mode,\n",
    "            early_stop_threshold=lr_find_early_stop_threshold\n",
    "        ),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    fn_trials = (\n",
    "        f\"model({model_name})_\"\n",
    "        f\"trials({n_trials}_{seed}_{n_startup_trials}_{n_ei_candidates})_\"\n",
    "        f\"tst({tst_split_id})_\"\n",
    "        f\"val({val_fold_id})\"\n",
    "    )\n",
    "\n",
    "    df_trials = pd.DataFrame(trials_results)\n",
    "    df_trials['split_id'] = tst_split_id\n",
    "    df_trials['fold_id'] = val_fold_id\n",
    "    df_trials[\"train_more\"] = False\n",
    "    df_trials.loc[(df_trials[\"train_loss\"] > df_trials[\"test_loss\"]) | (\n",
    "            df_trials[\"train_loss\"] > df_trials[\"validation_loss\"]), \"train_more\"] = True\n",
    "    df_trials[\"validation_test_mean_loss\"] = (df_trials[\"validation_loss\"] + df_trials[\"test_loss\"]) / 2.0\n",
    "    df_trials[\"train_validation_test_mean_loss\"] = (df_trials[\"train_loss\"] + df_trials[\"validation_loss\"] + df_trials[\"test_loss\"]) / 3.0\n",
    "    df_trials.style.background_gradient(cmap=\"RdYlGn_r\").to_excel(f\"{trainer_config['checkpoints_path']}/{fn_trials}.xlsx\")\n",
    "\n",
    "    dfs_models.append(df_trials)\n",
    "\n",
    "df_models = pd.concat(dfs_models, ignore_index=True)\n",
    "df_models.insert(0, 'Selected', 0)\n",
    "fn = (\n",
    "    f\"models_\"\n",
    "    f\"tst({tst_split_id})_\"\n",
    "    f\"val({val_fold_id})\"\n",
    ")\n",
    "df_models.style.background_gradient(cmap=\"RdYlGn_r\").to_excel(f\"{trainer_config['checkpoints_path']}/{fn}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform tests (High risk VS Low risk) on models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_tests = data.copy()\n",
    "\n",
    "fn_sweep = (\n",
    "    f\"models_\"\n",
    "    f\"tst({tst_split_id})_\"\n",
    "    f\"val({val_fold_id})\"\n",
    ")\n",
    "\n",
    "df_sweeps = pd.read_excel(f\"{path_ckpts}/{fn_sweep}.xlsx\", index_col=0)\n",
    "\n",
    "df_sweeps.insert(14, 'MW 1-2 p-value', None)\n",
    "df_sweeps.insert(15, 'MW 1-3 p-value', None)\n",
    "df_sweeps.insert(16, 'MW 2-3 p-value', None)\n",
    "df_sweeps.insert(17, 'Mean AA 1', None)\n",
    "df_sweeps.insert(18, 'Mean AA 2', None)\n",
    "df_sweeps.insert(19, 'Mean AA 3', None)\n",
    "\n",
    "models_tests_raw = pd.DataFrame(index=df_sweeps.index)\n",
    "for model_id, model_row in (pbar := tqdm(df_sweeps.iterrows())):\n",
    "    pbar.set_description(f\"Processing {model_id}\")\n",
    "\n",
    "    model_dir = str(pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).parent).replace('\\\\', '/') + '/' + pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).stem\n",
    "    \n",
    "    model = TabularModel.load_model(model_dir)\n",
    "\n",
    "    data_for_tests.loc[train.index, 'Group'] = 'Train'\n",
    "    data_for_tests.loc[validation.index, 'Group'] = 'Validation'\n",
    "    data_for_tests.loc[test.index, 'Group'] = 'Test'\n",
    "    data_for_tests['Prediction'] = model.predict(data_for_tests)\n",
    "    data_for_tests['Error'] = data_for_tests['Prediction'] - data_for_tests['Age']\n",
    "    corrector = LinearBiasCorrector()\n",
    "    corrector.fit(data_for_tests.loc[data_for_tests['Group'] == 'Train', feat_trgt].values, data_for_tests.loc[data_for_tests['Group'] == 'Train', 'Prediction'].values)\n",
    "    data_for_tests['Prediction Unbiased'] = corrector.predict(data_for_tests['Prediction'].values)\n",
    "    data_for_tests['Error Unbiased'] = data_for_tests['Prediction Unbiased'] - data_for_tests[feat_trgt]\n",
    "    \n",
    "    _, df_sweeps.at[model_id, 'MW 1-2 p-value'] = mannwhitneyu(\n",
    "        data_for_tests.loc[data_for_tests['Риск ССЗ'] == 'Низкий/умеренный', 'Error Unbiased'].values,\n",
    "        data_for_tests.loc[data_for_tests['Риск ССЗ'] == 'Высокий', 'Error Unbiased'].values,\n",
    "        alternative=\"two-sided\",\n",
    "    )\n",
    "    _, df_sweeps.at[model_id, 'MW 1-3 p-value'] = mannwhitneyu(\n",
    "        data_for_tests.loc[data_for_tests['Риск ССЗ'] == 'Низкий/умеренный', 'Error Unbiased'].values,\n",
    "        data_for_tests.loc[data_for_tests['Риск ССЗ'] == 'Очень высокий', 'Error Unbiased'].values,\n",
    "        alternative=\"two-sided\",\n",
    "    )\n",
    "    _, df_sweeps.at[model_id, 'MW 2-3 p-value'] = mannwhitneyu(\n",
    "        data_for_tests.loc[data_for_tests['Риск ССЗ'] == 'Высокий', 'Error Unbiased'].values,\n",
    "        data_for_tests.loc[data_for_tests['Риск ССЗ'] == 'Очень высокий', 'Error Unbiased'].values,\n",
    "        alternative=\"two-sided\",\n",
    "    )\n",
    "    df_sweeps.at[model_id, 'Mean AA 1'] = np.mean(data_for_tests.loc[data_for_tests['Риск ССЗ'] == 'Низкий/умеренный', 'Error Unbiased'])\n",
    "    df_sweeps.at[model_id, 'Mean AA 2'] = np.mean(data_for_tests.loc[data_for_tests['Риск ССЗ'] == 'Высокий', 'Error Unbiased'])\n",
    "    df_sweeps.at[model_id, 'Mean AA 3'] = np.mean(data_for_tests.loc[data_for_tests['Риск ССЗ'] == 'Очень высокий', 'Error Unbiased'])\n",
    "    \n",
    "df_sweeps.style.background_gradient(cmap=\"RdYlGn_r\").to_excel(f\"{path_ckpts}/models.xlsx\", index_label='Model ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best models analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_explain = True\n",
    "explain_method = \"GradientShap\"\n",
    "explain_baselines = \"b|1000\"\n",
    "explain_n_feats_to_plot = 25\n",
    "\n",
    "models_type = 'DANet'\n",
    "models_ids = [\n",
    "28,\n",
    "77,\n",
    "160,\n",
    "253,\n",
    "271,\n",
    "299,\n",
    "328,\n",
    "354,\n",
    "]\n",
    "models_ids = sorted(list(set(models_ids)))\n",
    "\n",
    "path_to_models = path_ckpts\n",
    "\n",
    "fn_sweep = (\n",
    "    f\"models\"\n",
    ")\n",
    "\n",
    "df_sweeps = pd.read_excel(f\"{path_to_models}/{fn_sweep}.xlsx\", index_col=0)\n",
    "\n",
    "path_to_candidates = f\"{path_to_models}/candidates/{models_type}\"\n",
    "pathlib.Path(path_to_candidates).mkdir(parents=True, exist_ok=True)\n",
    "df_sweeps.loc[models_ids, :].style.background_gradient(cmap=\"RdYlGn_r\"\n",
    ").to_excel(f\"{path_to_candidates}/selected.xlsx\")\n",
    "\n",
    "for model_id in models_ids:\n",
    "\n",
    "    model_dir = str(pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).parent).replace('\\\\', '/') + '/' + pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).stem\n",
    "    model = TabularModel.load_model(model_dir)\n",
    "    pathlib.Path(f\"{path_to_candidates}/{model_id}\").mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copytree(model_dir, f\"{path_to_candidates}/{model_id}\", dirs_exist_ok=True)\n",
    "    \n",
    "    df = data.loc[:, [feat_trgt, 'Сердечно-сосудистый риск']]\n",
    "    df.loc[train.index, 'Group'] = 'Train'\n",
    "    df.loc[validation.index, 'Group'] = 'Validation'\n",
    "    df.loc[test.index, 'Group'] = 'Test'\n",
    "    df['Prediction'] = model.predict(data)\n",
    "    df['Error'] = df['Prediction'] - df[feat_trgt]\n",
    "    corrector = LinearBiasCorrector()\n",
    "    corrector.fit(df.loc[df['Group'] == 'Train', feat_trgt].values, df.loc[df['Group'] == 'Train', 'Prediction'].values)\n",
    "    df['Prediction Unbiased'] = corrector.predict(df['Prediction'].values)\n",
    "    df['Error Unbiased'] = df['Prediction Unbiased'] - df[feat_trgt]\n",
    "    df.to_excel(f\"{path_to_candidates}/{model_id}/df.xlsx\")\n",
    "    \n",
    "    colors_groups = {\n",
    "        'Train': 'chartreuse',\n",
    "        'Validation': 'dodgerblue',\n",
    "        'Test': 'crimson',\n",
    "    }\n",
    "    \n",
    "    df_metrics = pd.DataFrame(\n",
    "        index=list(colors_groups.keys()),\n",
    "        columns=[\n",
    "            'mean_absolute_error', 'pearson_corrcoef', 'bias',\n",
    "            'mean_absolute_error_unbiased', 'pearson_corrcoef_unbiased', 'bias_unbiased'\n",
    "        ]\n",
    "    )\n",
    "    for group in colors_groups.keys():\n",
    "        pred = torch.from_numpy(df.loc[df['Group'] == group, 'Prediction'].values)\n",
    "        pred_unbiased = torch.from_numpy(df.loc[df['Group'] == group, 'Prediction Unbiased'].values)\n",
    "        real = torch.from_numpy(df.loc[df['Group'] == group, feat_trgt].values)\n",
    "        df_metrics.at[group, 'mean_absolute_error'] = mean_absolute_error(pred, real).numpy()\n",
    "        df_metrics.at[group, 'pearson_corrcoef'] = pearson_corrcoef(pred, real).numpy()\n",
    "        df_metrics.at[group, 'bias'] = np.mean(df.loc[df['Group'] == group, 'Error'].values)\n",
    "        df_metrics.at[group, 'mean_absolute_error_unbiased'] = mean_absolute_error(pred_unbiased, real).numpy()\n",
    "        df_metrics.at[group, 'pearson_corrcoef_unbiased'] = pearson_corrcoef(pred_unbiased, real).numpy()\n",
    "        df_metrics.at[group, 'bias_unbiased'] = np.mean(df.loc[df['Group'] == group, 'Error Unbiased'].values)\n",
    "    df_metrics.to_excel(f\"{path_to_candidates}/{model_id}/metrics.xlsx\", index_label=\"Metrics\")\n",
    "    \n",
    "    xy_min = df[[feat_trgt, 'Prediction']].min().min()\n",
    "    xy_max = df[[feat_trgt, 'Prediction']].max().max()\n",
    "    xy_ptp = xy_max - xy_min\n",
    "    \n",
    "    xy_min_unbiased = df[[feat_trgt, 'Prediction Unbiased']].min().min()\n",
    "    xy_max_unbiased = df[[feat_trgt, 'Prediction Unbiased']].max().max()\n",
    "    xy_ptp_unbiased = xy_max_unbiased - xy_min_unbiased\n",
    "    \n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "    for group in colors_groups.keys():    \n",
    "        regplot = sns.regplot(\n",
    "            data=df.loc[df['Group'] == group, :],\n",
    "            x=feat_trgt,\n",
    "            y=\"Prediction\",\n",
    "            label=group,\n",
    "            color=colors_groups[group],\n",
    "            scatter_kws=dict(\n",
    "                linewidth=0.2,\n",
    "                alpha=0.75,\n",
    "                edgecolor=\"k\",\n",
    "                s=20,\n",
    "            ),\n",
    "            ax=ax\n",
    "        )\n",
    "    bisect = sns.lineplot(\n",
    "        x=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        y=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        linestyle='--',\n",
    "        color='black',\n",
    "        linewidth=1.0,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params)\")\n",
    "    ax.set_xlim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    ax.set_ylim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "    for group in colors_groups.keys():    \n",
    "        regplot = sns.regplot(\n",
    "            data=df.loc[df['Group'] == group, :],\n",
    "            x=feat_trgt,\n",
    "            y=\"Prediction Unbiased\",\n",
    "            label=group,\n",
    "            color=colors_groups[group],\n",
    "            scatter_kws=dict(\n",
    "                linewidth=0.2,\n",
    "                alpha=0.75,\n",
    "                edgecolor=\"k\",\n",
    "                s=20,\n",
    "            ),\n",
    "            ax=ax\n",
    "        )\n",
    "    bisect = sns.lineplot(\n",
    "        x=[xy_min_unbiased - 0.1 * xy_ptp_unbiased, xy_max_unbiased + 0.1 * xy_ptp_unbiased],\n",
    "        y=[xy_min_unbiased - 0.1 * xy_ptp_unbiased, xy_max_unbiased + 0.1 * xy_ptp_unbiased],\n",
    "        linestyle='--',\n",
    "        color='black',\n",
    "        linewidth=1.0,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params)\")\n",
    "    ax.set_xlim(xy_min_unbiased - 0.1 * xy_ptp, xy_max_unbiased + 0.1 * xy_ptp_unbiased)\n",
    "    ax.set_ylim(xy_min_unbiased - 0.1 * xy_ptp, xy_max_unbiased + 0.1 * xy_ptp_unbiased)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot_unbiased.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot_unbiased.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))   \n",
    "    scatter = sns.scatterplot(\n",
    "        data=df,\n",
    "        x=feat_trgt,\n",
    "        y=\"Prediction\",\n",
    "        hue=\"Group\",\n",
    "        palette=colors_groups,\n",
    "        linewidth=0.2,\n",
    "        alpha=0.75,\n",
    "        edgecolor=\"k\",\n",
    "        s=20,\n",
    "        hue_order=list(colors_groups.keys()),\n",
    "        ax=ax\n",
    "    )\n",
    "    bisect = sns.lineplot(\n",
    "        x=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        y=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        linestyle='--',\n",
    "        color='black',\n",
    "        linewidth=1.0,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params)\")\n",
    "    ax.set_xlim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    ax.set_ylim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/scatter.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/scatter.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    df_fig = df.loc[:, ['Error', 'Group']]\n",
    "    groups_rename = {\n",
    "        group: f\"{group}\" + \"\\n\" +\n",
    "               fr\"MAE: {df_metrics.at[group, 'mean_absolute_error']:0.2f}\" + \"\\n\"\n",
    "               fr\"Pearson $\\rho$: {df_metrics.at[group, 'pearson_corrcoef']:0.2f}\" + \"\\n\" +\n",
    "               fr\"$\\langle$Error$\\rangle$: {df_metrics.at[group, 'bias']:0.2f}\" \n",
    "        for group in colors_groups\n",
    "    }\n",
    "    colors_groups_violin = {groups_rename[group]: colors_groups[group] for group in colors_groups}\n",
    "    df_fig['Group'].replace(groups_rename, inplace=True)\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    violin = sns.violinplot(\n",
    "        data=df_fig,\n",
    "        x='Group',\n",
    "        y='Error',\n",
    "        palette=colors_groups_violin,\n",
    "        scale='width',\n",
    "        order=list(colors_groups_violin.keys()),\n",
    "        saturation=0.75,\n",
    "        legend=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel('')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    df_fig = df.loc[:, ['Error Unbiased', 'Group']]\n",
    "    groups_rename = {\n",
    "        group: f\"{group}\" + \"\\n\" +\n",
    "               fr\"MAE: {df_metrics.at[group, 'mean_absolute_error_unbiased']:0.2f}\" + \"\\n\"\n",
    "               fr\"Pearson $\\rho$: {df_metrics.at[group, 'pearson_corrcoef_unbiased']:0.2f}\" + \"\\n\" +\n",
    "               fr\"$\\langle$Error$\\rangle$: {df_metrics.at[group, 'bias_unbiased']:0.2f}\" \n",
    "        for group in colors_groups\n",
    "    }\n",
    "    colors_groups_violin = {groups_rename[group]: colors_groups[group] for group in colors_groups}\n",
    "    df_fig['Group'].replace(groups_rename, inplace=True)\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    violin = sns.violinplot(\n",
    "        data=df_fig,\n",
    "        x='Group',\n",
    "        y='Error Unbiased',\n",
    "        palette=colors_groups_violin,\n",
    "        scale='width',\n",
    "        order=list(colors_groups_violin.keys()),\n",
    "        saturation=0.75,\n",
    "        legend=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel('')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin_unbiased.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin_unbiased.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    dataparts_ids = ['Низкий/умеренный', 'Высокий', 'Очень высокий']\n",
    "    dataparts_colors = {'Низкий/умеренный': 'chartreuse', 'Высокий': 'gold', 'Очень высокий': 'crimson'}\n",
    "    df_fig = df.copy()\n",
    "    plot_dataparts_count = df_fig['Сердечно-сосудистый риск'].value_counts()\n",
    "    plot_dataparts_rename = {}\n",
    "    for x in dataparts_ids:\n",
    "        plot_dataparts_rename[x] = x + f\"\\nCount: {plot_dataparts_count[x]}\\nBias: {np.mean(df_fig.loc[df_fig['Сердечно-сосудистый риск'] == x, 'Error Unbiased']):0.1f}\"\n",
    "    plot_dataparts = [plot_dataparts_rename[x] for x in dataparts_ids]\n",
    "    plot_pairs = [\n",
    "        ('Низкий/умеренный', 'Высокий'),\n",
    "        ('Низкий/умеренный', 'Очень высокий'),\n",
    "        ('Высокий', 'Очень высокий'),\n",
    "    ]\n",
    "    plot_pairs = [(plot_dataparts_rename[x[0]], plot_dataparts_rename[x[1]]) for x in plot_pairs]\n",
    "    df_fig['Сердечно-сосудистый риск'] = df_fig['Сердечно-сосудистый риск'].replace(plot_dataparts_rename)\n",
    "    colors_plot_dataparts = {plot_dataparts_rename[x]: dataparts_colors[x] for x in plot_dataparts_rename}\n",
    "    pvals_formatted = []\n",
    "    for plot_pair in plot_pairs:\n",
    "        pval = mannwhitneyu(\n",
    "            df_fig.loc[df_fig['Сердечно-сосудистый риск'] == plot_pair[0], f'Error Unbiased'].values,\n",
    "            df_fig.loc[df_fig['Сердечно-сосудистый риск'] == plot_pair[1], f'Error Unbiased'].values,\n",
    "            alternative='two-sided'\n",
    "        ).pvalue\n",
    "        pvals_formatted.append(f\"{pval:.1e}\")\n",
    "    \n",
    "    sns.set_theme(style='ticks')\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    violin = sns.violinplot(\n",
    "        data=df_fig,\n",
    "        x='Сердечно-сосудистый риск',\n",
    "        y='Error Unbiased',\n",
    "        hue='Сердечно-сосудистый риск',\n",
    "        hue_order=plot_dataparts,\n",
    "        palette=colors_plot_dataparts,\n",
    "        density_norm='width',\n",
    "        saturation=0.75,\n",
    "        linewidth=1.0,\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "    annotator = Annotator(\n",
    "        ax=ax,\n",
    "        pairs=plot_pairs,\n",
    "        data=df_fig,\n",
    "        x=\"Сердечно-сосудистый риск\",\n",
    "        y='Error Unbiased',\n",
    "        order=plot_dataparts,\n",
    "    )\n",
    "    annotator.set_custom_annotations(pvals_formatted)\n",
    "    annotator.configure(loc='inside', verbose=0)\n",
    "    annotator.annotate()\n",
    "    ax.set_ylabel('Age Acceleration')\n",
    "    ax.set_xlabel('')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violins_global.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violins_global.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        explanation = model.explain(data, method=explain_method, baselines=explain_baselines)\n",
    "        explanation.index = data.index\n",
    "        explanation.to_excel(f\"{path_to_candidates}/{model_id}/explanation.xlsx\")\n",
    "        \n",
    "        # sns.set_theme(style='whitegrid')\n",
    "        # fig = shap.summary_plot(\n",
    "        #     shap_values=explanation.loc[:, feats].values,\n",
    "        #     features=data.loc[:, feats].values,\n",
    "        #     feature_names=feats,\n",
    "        #     max_display=explain_n_feats_to_plot,\n",
    "        #     plot_type=\"violin\",\n",
    "        #     show=False,\n",
    "        # )\n",
    "        # plt.savefig(f\"{path_to_candidates}/{model_id}/explain_beeswarm.png\", bbox_inches='tight', dpi=200)\n",
    "        # plt.savefig(f\"{path_to_candidates}/{model_id}/explain_beeswarm.pdf\", bbox_inches='tight')\n",
    "        # plt.close(fig)\n",
    "        \n",
    "        sns.set_theme(style='ticks')\n",
    "        fig = shap.summary_plot(\n",
    "            shap_values=explanation.loc[:, feats].values,\n",
    "            features=data.loc[:, feats].values,\n",
    "            feature_names=feats,\n",
    "            max_display=explain_n_feats_to_plot,\n",
    "            plot_type=\"bar\",\n",
    "            show=False,\n",
    "            plot_size=[12,8]\n",
    "        )\n",
    "        plt.savefig(f\"{path_to_candidates}/{model_id}/explain_bar.png\", bbox_inches='tight', dpi=200)\n",
    "        plt.savefig(f\"{path_to_candidates}/{model_id}/explain_bar.pdf\", bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    \n",
    "    except NotImplementedError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best models with SHAP correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_explain = True\n",
    "explain_method = \"GradientShap\"\n",
    "explain_baselines = \"b|1000\"\n",
    "explain_n_feats_to_plot = 25\n",
    "\n",
    "models_type = 'DANet'\n",
    "models_ids = [\n",
    "288,\n",
    "294,\n",
    "309,\n",
    "349,\n",
    "391,\n",
    "443,\n",
    "451,\n",
    "459,\n",
    "465,\n",
    "482,\n",
    "493,\n",
    "]\n",
    "models_ids = sorted(list(set(models_ids)))\n",
    "\n",
    "ids_shap_check = test.index.values\n",
    "bkg_count = 10\n",
    "\n",
    "feat_trgt = 'Age'\n",
    "data_for_shap = data.copy()\n",
    "data_for_shap.loc[train.index, 'Group'] = 'Train'\n",
    "data_for_shap.loc[validation.index, 'Group'] = 'Validation'\n",
    "data_for_shap.loc[test.index, 'Group'] = 'Test'\n",
    "\n",
    "feats_corr = pd.DataFrame(index=feats, columns=['Correlation'])\n",
    "for f in feats:\n",
    "    feats_corr.at[f, 'Correlation'], _ = stats.pearsonr(data.loc[:, f].values, data.loc[:, feat_trgt].values)\n",
    "\n",
    "path_to_models = path_ckpts\n",
    "fn_sweep = (\n",
    "    f\"models\"\n",
    ")\n",
    "df_sweeps = pd.read_excel(f\"{path_to_models}/{fn_sweep}.xlsx\", index_col=0)\n",
    "\n",
    "path_to_candidates = f\"{path_to_models}/candidates/{models_type}\"\n",
    "pathlib.Path(path_to_candidates).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "colors_groups = {\n",
    "    'Train': 'chartreuse',\n",
    "    'Validation': 'dodgerblue',\n",
    "    'Test': 'crimson',\n",
    "}\n",
    "\n",
    "for model_id in models_ids:\n",
    "\n",
    "    model_dir = str(pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).parent).replace('\\\\', '/') + '/' + pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).stem\n",
    "    model = TabularModel.load_model(model_dir)\n",
    "    pathlib.Path(f\"{path_to_candidates}/{model_id}\").mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copytree(model_dir, f\"{path_to_candidates}/{model_id}\", dirs_exist_ok=True)\n",
    "    \n",
    "    df = data.loc[:, [feat_trgt, 'Риск ССЗ']]\n",
    "    df.loc[train.index, 'Group'] = 'Train'\n",
    "    df.loc[validation.index, 'Group'] = 'Validation'\n",
    "    df.loc[test.index, 'Group'] = 'Test'\n",
    "    df['Prediction'] = model.predict(data)\n",
    "    df['Error'] = df['Prediction'] - df[feat_trgt]\n",
    "    corrector = LinearBiasCorrector()\n",
    "    corrector.fit(df.loc[df['Group'] == 'Train', feat_trgt].values, df.loc[df['Group'] == 'Train', 'Prediction'].values)\n",
    "    df['Prediction Unbiased'] = corrector.predict(df['Prediction'].values)\n",
    "    df['Error Unbiased'] = df['Prediction Unbiased'] - df[feat_trgt]\n",
    "    df.to_excel(f\"{path_to_candidates}/{model_id}/df.xlsx\")\n",
    "    \n",
    "    def predict_func(X):\n",
    "        X_df = pd.DataFrame(data=X, columns=feats)\n",
    "        y = model.predict(X_df)[f'{feat_trgt}_prediction'].values\n",
    "        y = corrector.predict(y)\n",
    "        return y\n",
    "    \n",
    "    data_for_shap['Prediction'] = df['Prediction']\n",
    "    data_for_shap['Error'] = df['Error']\n",
    "    data_for_shap['Prediction Unbiased'] = df['Prediction Unbiased']\n",
    "    data_for_shap['Error Unbiased'] = df['Error Unbiased']\n",
    "    \n",
    "    df_metrics = pd.DataFrame(\n",
    "        index=list(colors_groups.keys()),\n",
    "        columns=[\n",
    "            'mean_absolute_error', 'pearson_corrcoef', 'bias',\n",
    "            'mean_absolute_error_unbiased', 'pearson_corrcoef_unbiased', 'bias_unbiased'\n",
    "        ]\n",
    "    )\n",
    "    for group in colors_groups.keys():\n",
    "        pred = torch.from_numpy(df.loc[df['Group'] == group, 'Prediction'].values)\n",
    "        pred_unbiased = torch.from_numpy(df.loc[df['Group'] == group, 'Prediction Unbiased'].values)\n",
    "        real = torch.from_numpy(df.loc[df['Group'] == group, feat_trgt].values)\n",
    "        df_metrics.at[group, 'mean_absolute_error'] = mean_absolute_error(pred, real).numpy()\n",
    "        df_metrics.at[group, 'pearson_corrcoef'] = pearson_corrcoef(pred, real).numpy()\n",
    "        df_metrics.at[group, 'bias'] = np.mean(df.loc[df['Group'] == group, 'Error'].values)\n",
    "        df_metrics.at[group, 'mean_absolute_error_unbiased'] = mean_absolute_error(pred_unbiased, real).numpy()\n",
    "        df_metrics.at[group, 'pearson_corrcoef_unbiased'] = pearson_corrcoef(pred_unbiased, real).numpy()\n",
    "        df_metrics.at[group, 'bias_unbiased'] = np.mean(df.loc[df['Group'] == group, 'Error Unbiased'].values)\n",
    "    df_metrics.to_excel(f\"{path_to_candidates}/{model_id}/metrics.xlsx\", index_label=\"Metrics\")\n",
    "    \n",
    "    mae = df_metrics.at['Test', 'mean_absolute_error_unbiased']\n",
    "    rho = df_metrics.at['Test', 'pearson_corrcoef_unbiased']\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df_correspondence = pd.DataFrame()\n",
    "    for sample_id in ids_shap_check:\n",
    "        \n",
    "        trgt_age = data_for_shap.at[sample_id, feat_trgt]\n",
    "        trgt_pred_raw = data_for_shap.at[sample_id, 'Prediction Unbiased']\n",
    "        \n",
    "        data_closest = data_for_shap.loc[data_for_shap['Error Unbiased'].abs() < mae * rho * feats_corr['Correlation'].abs().max(), :]\n",
    "        data_closest = data_closest.iloc[(data_closest['Prediction Unbiased'] - trgt_age).abs().argsort()[:bkg_count]]\n",
    "        \n",
    "        print(f\"Background count: {data_closest.shape[0]}\")\n",
    "        print(f\"Background min diff: {(data_closest['Prediction Unbiased'] - trgt_age).min()}\")\n",
    "        print(f\"Background max diff: {(data_closest['Prediction Unbiased'] - trgt_age).max()}\")\n",
    "        \n",
    "        explainer = shap.SamplingExplainer(predict_func, data_closest.loc[:, feats].values)\n",
    "        shap_values = explainer.shap_values(data_for_shap.loc[[sample_id], feats].values)[0]\n",
    "        shap_values = shap_values * (trgt_pred_raw - trgt_age) / (trgt_pred_raw - explainer.expected_value)\n",
    "        print(f\"SHAP values difference: {sum(shap_values) - (trgt_pred_raw - trgt_age)}\")\n",
    "        \n",
    "        # SHAP values correction 1\n",
    "        shap_corr_thld = 3.0\n",
    "        shap_corr_to = 1.0\n",
    "        shap_mean_abs = np.mean(np.abs(shap_values))\n",
    "        if shap_mean_abs > shap_corr_thld and abs(sum(shap_values)) < shap_corr_thld:\n",
    "            print('SHAP values correction 1')\n",
    "            shap_pos_ids = np.argwhere(shap_values >= 0).ravel()\n",
    "            shap_neg_ids = np.argwhere(shap_values < 0).ravel()\n",
    "            \n",
    "            shap_pos_sum_abs = np.sum(np.abs(shap_values[shap_pos_ids]))\n",
    "            shap_neg_sum_abs = np.sum(np.abs(shap_values[shap_neg_ids]))\n",
    "            \n",
    "            shap_sum_abs_from = np.sum(np.abs(shap_values))\n",
    "            shap_sum_abs_to = shap_corr_to * len(shap_values)\n",
    "            \n",
    "            shap_corr_diff = shap_sum_abs_from - shap_sum_abs_to\n",
    "            \n",
    "            for pos_id in shap_pos_ids:\n",
    "                curr_part = abs(shap_values[pos_id]) / shap_pos_sum_abs\n",
    "                shap_values[pos_id] -= curr_part * shap_corr_diff * 0.5\n",
    "            for neg_id in shap_neg_ids:\n",
    "                curr_part = abs(shap_values[neg_id]) / shap_neg_sum_abs\n",
    "                shap_values[neg_id] += curr_part * shap_corr_diff * 0.5\n",
    "                \n",
    "        # SHAP values correction 2\n",
    "        shap_corr_max_thld = 1.0\n",
    "        shap_corr_max_to = 0.95\n",
    "        shap_max_abs = np.max(np.abs(shap_values))\n",
    "        shap_abs_sum = np.abs(np.sum(shap_values))\n",
    "        if shap_max_abs > shap_corr_max_thld * shap_abs_sum:\n",
    "            print('SHAP values correction 2')\n",
    "            shap_pos_ids = np.argwhere(shap_values >= 0).ravel()\n",
    "            shap_neg_ids = np.argwhere(shap_values < 0).ravel()\n",
    "            \n",
    "            shap_pos_sum_abs = np.sum(np.abs(shap_values[shap_pos_ids]))\n",
    "            shap_neg_sum_abs = np.sum(np.abs(shap_values[shap_neg_ids]))\n",
    "            \n",
    "            shap_corr_diff = (shap_max_abs - shap_corr_max_thld) * shap_corr_max_to\n",
    "            \n",
    "            for pos_id in shap_pos_ids:\n",
    "                curr_part = abs(shap_values[pos_id]) / shap_pos_sum_abs\n",
    "                shap_values[pos_id] -= curr_part * shap_corr_diff\n",
    "            for neg_id in shap_neg_ids:\n",
    "                curr_part = abs(shap_values[neg_id]) / shap_neg_sum_abs\n",
    "                shap_values[neg_id] += curr_part * shap_corr_diff\n",
    "        \n",
    "        df_comp = pd.DataFrame(index=feats, columns=['SHAP', 'Values', 'Correlation', 'Percentile', 'Consistent'])\n",
    "        df_comp['SHAP'] = shap_values\n",
    "        df_comp.sort_values(by='SHAP', key=abs, inplace=True)\n",
    "        df_comp.loc[df_comp.index.values, 'Values'] = data_for_shap.loc[sample_id, df_comp.index.values].values\n",
    "        df_comp.loc[df_comp.index.values, 'Correlation'] = feats_corr.loc[df_comp.index.values, 'Correlation']\n",
    "        for f_id, f in enumerate(df_comp.index.values):\n",
    "            df_comp.at[f, 'Percentile'] = stats.percentileofscore(data_closest.loc[:, f].values, data_for_shap.at[sample_id, f])\n",
    "            if (\n",
    "                ((df_comp.at[f, 'Correlation'] > 0) & (df_comp.at[f, 'SHAP'] > 0) & (df_comp.at[f, 'Percentile'] > 55)) or \\\n",
    "                ((df_comp.at[f, 'Correlation'] > 0) & (df_comp.at[f, 'SHAP'] < 0) & (df_comp.at[f, 'Percentile'] < 45)) or \\\n",
    "                ((df_comp.at[f, 'Correlation'] < 0) & (df_comp.at[f, 'SHAP'] > 0) & (df_comp.at[f, 'Percentile'] < 45)) or \\\n",
    "                ((df_comp.at[f, 'Correlation'] < 0) & (df_comp.at[f, 'SHAP'] < 0) & (df_comp.at[f, 'Percentile'] > 55))\n",
    "                ):\n",
    "                df_comp.at[f, 'Consistent'] = 1\n",
    "            else:\n",
    "                df_comp.at[f, 'Consistent'] = 0\n",
    "\n",
    "            df_correspondence.at[sample_id, f\"{f} Consistent\"] = df_comp.at[f, 'Consistent']\n",
    "            df_correspondence.at[sample_id, f\"{f} Order\"] = f_id\n",
    "            df_correspondence.at[sample_id, f\"{f} Percentile\"] = df_comp.at[f, 'Percentile']\n",
    "            df_correspondence.at[sample_id, f\"{f} SHAP\"] = df_comp.at[f, 'SHAP']\n",
    "            df_correspondence.at[sample_id, f\"{f} Value\"] = df_comp.at[f, 'Values']\n",
    "            df_correspondence.at[sample_id, f\"{f} Correlation\"] = df_comp.at[f, 'Correlation']\n",
    "            \n",
    "    for n_top_feats in np.arange(1, len(feats) + 1):\n",
    "        df_sweeps.at[model_id, f\"SHAP top-{n_top_feats}\"] = 0.0\n",
    "        for f in feats:\n",
    "            df_sweeps.at[model_id, f\"SHAP top-{n_top_feats}\"] += df_correspondence.loc[df_correspondence[f\"{f} Order\"] > len(feats) - n_top_feats - 1, f\"{f} Consistent\"].sum()\n",
    "        df_sweeps.at[model_id, f\"SHAP top-{n_top_feats}\"] /= (n_top_feats * len(ids_shap_check))\n",
    "        \n",
    "    df_correspondence.to_excel(f\"{path_to_candidates}/{model_id}/correspondence.xlsx\")\n",
    "    \n",
    "    \n",
    "    xy_min = df[[feat_trgt, 'Prediction']].min().min()\n",
    "    xy_max = df[[feat_trgt, 'Prediction']].max().max()\n",
    "    xy_ptp = xy_max - xy_min\n",
    "    \n",
    "    xy_min_unbiased = df[[feat_trgt, 'Prediction Unbiased']].min().min()\n",
    "    xy_max_unbiased = df[[feat_trgt, 'Prediction Unbiased']].max().max()\n",
    "    xy_ptp_unbiased = xy_max_unbiased - xy_min_unbiased\n",
    "    \n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "    for group in colors_groups.keys():    \n",
    "        regplot = sns.regplot(\n",
    "            data=df.loc[df['Group'] == group, :],\n",
    "            x=feat_trgt,\n",
    "            y=\"Prediction\",\n",
    "            label=group,\n",
    "            color=colors_groups[group],\n",
    "            scatter_kws=dict(\n",
    "                linewidth=0.2,\n",
    "                alpha=0.75,\n",
    "                edgecolor=\"k\",\n",
    "                s=20,\n",
    "            ),\n",
    "            ax=ax\n",
    "        )\n",
    "    bisect = sns.lineplot(\n",
    "        x=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        y=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        linestyle='--',\n",
    "        color='black',\n",
    "        linewidth=1.0,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params)\")\n",
    "    ax.set_xlim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    ax.set_ylim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "    for group in colors_groups.keys():    \n",
    "        regplot = sns.regplot(\n",
    "            data=df.loc[df['Group'] == group, :],\n",
    "            x=feat_trgt,\n",
    "            y=\"Prediction Unbiased\",\n",
    "            label=group,\n",
    "            color=colors_groups[group],\n",
    "            scatter_kws=dict(\n",
    "                linewidth=0.2,\n",
    "                alpha=0.75,\n",
    "                edgecolor=\"k\",\n",
    "                s=20,\n",
    "            ),\n",
    "            ax=ax\n",
    "        )\n",
    "    bisect = sns.lineplot(\n",
    "        x=[xy_min_unbiased - 0.1 * xy_ptp_unbiased, xy_max_unbiased + 0.1 * xy_ptp_unbiased],\n",
    "        y=[xy_min_unbiased - 0.1 * xy_ptp_unbiased, xy_max_unbiased + 0.1 * xy_ptp_unbiased],\n",
    "        linestyle='--',\n",
    "        color='black',\n",
    "        linewidth=1.0,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params)\")\n",
    "    ax.set_xlim(xy_min_unbiased - 0.1 * xy_ptp, xy_max_unbiased + 0.1 * xy_ptp_unbiased)\n",
    "    ax.set_ylim(xy_min_unbiased - 0.1 * xy_ptp, xy_max_unbiased + 0.1 * xy_ptp_unbiased)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot_unbiased.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot_unbiased.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))   \n",
    "    scatter = sns.scatterplot(\n",
    "        data=df,\n",
    "        x=feat_trgt,\n",
    "        y=\"Prediction\",\n",
    "        hue=\"Group\",\n",
    "        palette=colors_groups,\n",
    "        linewidth=0.2,\n",
    "        alpha=0.75,\n",
    "        edgecolor=\"k\",\n",
    "        s=20,\n",
    "        hue_order=list(colors_groups.keys()),\n",
    "        ax=ax\n",
    "    )\n",
    "    bisect = sns.lineplot(\n",
    "        x=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        y=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        linestyle='--',\n",
    "        color='black',\n",
    "        linewidth=1.0,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params)\")\n",
    "    ax.set_xlim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    ax.set_ylim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/scatter.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/scatter.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    df_fig = df.loc[:, ['Error', 'Group']]\n",
    "    groups_rename = {\n",
    "        group: f\"{group}\" + \"\\n\" +\n",
    "               fr\"MAE: {df_metrics.at[group, 'mean_absolute_error']:0.2f}\" + \"\\n\"\n",
    "               fr\"Pearson $\\rho$: {df_metrics.at[group, 'pearson_corrcoef']:0.2f}\" + \"\\n\" +\n",
    "               fr\"$\\langle$Error$\\rangle$: {df_metrics.at[group, 'bias']:0.2f}\" \n",
    "        for group in colors_groups\n",
    "    }\n",
    "    colors_groups_violin = {groups_rename[group]: colors_groups[group] for group in colors_groups}\n",
    "    df_fig['Group'].replace(groups_rename, inplace=True)\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    violin = sns.violinplot(\n",
    "        data=df_fig,\n",
    "        x='Group',\n",
    "        y='Error',\n",
    "        palette=colors_groups_violin,\n",
    "        scale='width',\n",
    "        order=list(colors_groups_violin.keys()),\n",
    "        saturation=0.75,\n",
    "        legend=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel('')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    df_fig = df.loc[:, ['Error Unbiased', 'Group']]\n",
    "    groups_rename = {\n",
    "        group: f\"{group}\" + \"\\n\" +\n",
    "               fr\"MAE: {df_metrics.at[group, 'mean_absolute_error_unbiased']:0.2f}\" + \"\\n\"\n",
    "               fr\"Pearson $\\rho$: {df_metrics.at[group, 'pearson_corrcoef_unbiased']:0.2f}\" + \"\\n\" +\n",
    "               fr\"$\\langle$Error$\\rangle$: {df_metrics.at[group, 'bias_unbiased']:0.2f}\" \n",
    "        for group in colors_groups\n",
    "    }\n",
    "    colors_groups_violin = {groups_rename[group]: colors_groups[group] for group in colors_groups}\n",
    "    df_fig['Group'].replace(groups_rename, inplace=True)\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    violin = sns.violinplot(\n",
    "        data=df_fig,\n",
    "        x='Group',\n",
    "        y='Error Unbiased',\n",
    "        palette=colors_groups_violin,\n",
    "        scale='width',\n",
    "        order=list(colors_groups_violin.keys()),\n",
    "        saturation=0.75,\n",
    "        legend=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel('')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin_unbiased.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin_unbiased.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    dataparts_ids = ['Низкий/умеренный', 'Высокий', 'Очень высокий']\n",
    "    dataparts_colors = {'Низкий/умеренный': 'chartreuse', 'Высокий': 'gold', 'Очень высокий': 'crimson'}\n",
    "    df_fig = df.copy()\n",
    "    plot_dataparts_count = df_fig['Риск ССЗ'].value_counts()\n",
    "    plot_dataparts_rename = {}\n",
    "    for x in dataparts_ids:\n",
    "        plot_dataparts_rename[x] = x + f\"\\nCount: {plot_dataparts_count[x]}\\nBias: {np.mean(df_fig.loc[df_fig['Риск ССЗ'] == x, 'Error Unbiased']):0.1f}\"\n",
    "    plot_dataparts = [plot_dataparts_rename[x] for x in dataparts_ids]\n",
    "    plot_pairs = [\n",
    "        ('Низкий/умеренный', 'Высокий'),\n",
    "        ('Низкий/умеренный', 'Очень высокий'),\n",
    "        ('Высокий', 'Очень высокий'),\n",
    "    ]\n",
    "    plot_pairs = [(plot_dataparts_rename[x[0]], plot_dataparts_rename[x[1]]) for x in plot_pairs]\n",
    "    df_fig['Риск ССЗ'] = df_fig['Риск ССЗ'].replace(plot_dataparts_rename)\n",
    "    colors_plot_dataparts = {plot_dataparts_rename[x]: dataparts_colors[x] for x in plot_dataparts_rename}\n",
    "    pvals_formatted = []\n",
    "    for plot_pair in plot_pairs:\n",
    "        pval = mannwhitneyu(\n",
    "            df_fig.loc[df_fig['Риск ССЗ'] == plot_pair[0], f'Error Unbiased'].values,\n",
    "            df_fig.loc[df_fig['Риск ССЗ'] == plot_pair[1], f'Error Unbiased'].values,\n",
    "            alternative='two-sided'\n",
    "        ).pvalue\n",
    "        pvals_formatted.append(f\"{pval:.1e}\")\n",
    "    \n",
    "    sns.set_theme(style='ticks')\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    violin = sns.violinplot(\n",
    "        data=df_fig,\n",
    "        x='Риск ССЗ',\n",
    "        y='Error Unbiased',\n",
    "        hue='Риск ССЗ',\n",
    "        hue_order=plot_dataparts,\n",
    "        palette=colors_plot_dataparts,\n",
    "        density_norm='width',\n",
    "        saturation=0.75,\n",
    "        linewidth=1.0,\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "    )\n",
    "    annotator = Annotator(\n",
    "        ax=ax,\n",
    "        pairs=plot_pairs,\n",
    "        data=df_fig,\n",
    "        x=\"Риск ССЗ\",\n",
    "        y='Error Unbiased',\n",
    "        order=plot_dataparts,\n",
    "    )\n",
    "    annotator.set_custom_annotations(pvals_formatted)\n",
    "    annotator.configure(loc='inside', verbose=0)\n",
    "    annotator.annotate()\n",
    "    ax.set_ylabel('Age Acceleration')\n",
    "    ax.set_xlabel('')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violins_global.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violins_global.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        explanation = model.explain(data, method=explain_method, baselines=explain_baselines)\n",
    "        explanation.index = data.index\n",
    "        explanation.to_excel(f\"{path_to_candidates}/{model_id}/explanation.xlsx\")\n",
    "        \n",
    "        # sns.set_theme(style='whitegrid')\n",
    "        # fig = shap.summary_plot(\n",
    "        #     shap_values=explanation.loc[:, feats].values,\n",
    "        #     features=data.loc[:, feats].values,\n",
    "        #     feature_names=feats,\n",
    "        #     max_display=explain_n_feats_to_plot,\n",
    "        #     plot_type=\"violin\",\n",
    "        #     show=False,\n",
    "        # )\n",
    "        # plt.savefig(f\"{path_to_candidates}/{model_id}/explain_beeswarm.png\", bbox_inches='tight', dpi=200)\n",
    "        # plt.savefig(f\"{path_to_candidates}/{model_id}/explain_beeswarm.pdf\", bbox_inches='tight')\n",
    "        # plt.close(fig)\n",
    "        \n",
    "        sns.set_theme(style='ticks')\n",
    "        fig = shap.summary_plot(\n",
    "            shap_values=explanation.loc[:, feats].values,\n",
    "            features=data.loc[:, feats].values,\n",
    "            feature_names=feats,\n",
    "            max_display=explain_n_feats_to_plot,\n",
    "            plot_type=\"bar\",\n",
    "            show=False,\n",
    "            plot_size=[12,8]\n",
    "        )\n",
    "        plt.savefig(f\"{path_to_candidates}/{model_id}/explain_bar.png\", bbox_inches='tight', dpi=200)\n",
    "        plt.savefig(f\"{path_to_candidates}/{model_id}/explain_bar.pdf\", bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    \n",
    "    except NotImplementedError:\n",
    "        pass\n",
    "    \n",
    "df_sweeps.loc[models_ids, :].style.background_gradient(cmap=\"RdYlGn_r\").to_excel(f\"{path_to_candidates}/selected.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
