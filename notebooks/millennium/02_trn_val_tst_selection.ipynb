{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Debugging autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T14:29:22.801727Z",
     "start_time": "2024-07-01T14:29:18.695223Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pytorch_tabular.utils import load_covertype_dataset\n",
    "from rich.pretty import pprint\n",
    "from sklearn.model_selection import BaseCrossValidator, ParameterGrid, ParameterSampler\n",
    "import torch\n",
    "import pickle\n",
    "import shutil\n",
    "import shap\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from glob import glob\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pytorch_tabular.utils import make_mixed_dataset, print_metrics\n",
    "from pytorch_tabular import available_models\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig, GANDALFConfig, TabNetModelConfig, FTTransformerConfig, DANetConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from pytorch_tabular.tabular_model_tuner import TabularModelTuner\n",
    "from torchmetrics.functional.regression import mean_absolute_error, pearson_corrcoef\n",
    "from pytorch_tabular import MODEL_SWEEP_PRESETS\n",
    "import pandas as pd\n",
    "from pytorch_tabular import model_sweep\n",
    "from src.pt.model_sweep import model_sweep_custom\n",
    "import warnings\n",
    "from src.utils.configs import read_parse_config\n",
    "from src.utils.hash import dict_hash\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import distinctipy\n",
    "import matplotlib.patheffects as pe\n",
    "import matplotlib.colors as mcolors\n",
    "from statannotations.Annotator import Annotator\n",
    "from scipy.stats import mannwhitneyu\n",
    "from regression_bias_corrector import LinearBiasCorrector\n",
    "import missingno as msno\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from sklearn.manifold import MDS, Isomap, TSNE\n",
    "from sklearn.cluster import DBSCAN, HDBSCAN\n",
    "\n",
    "\n",
    "def make_rgb_transparent(rgb, bg_rgb, alpha):\n",
    "    return [alpha * c1 + (1 - alpha) * c2 for (c1, c2) in zip(rgb, bg_rgb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_set = 'Электрокардиограмма (чекап)'\n",
    "feats_title = 'Электрокардиограмма'\n",
    "\n",
    "path = f\"E:/YandexDisk/Work/bbd/millennium/models/{feats_set}\"\n",
    "path_models = f\"E:/Git/bbs/notebooks/millennium/pt/{feats_set}\"\n",
    "\n",
    "tst_n_splits = 5\n",
    "tst_n_repeats = 5\n",
    "tst_random_state = 1337\n",
    "\n",
    "val_n_splits = 4\n",
    "val_n_repeats = 4\n",
    "val_random_state = 1337\n",
    "\n",
    "data = pd.read_excel(f\"{path}/data.xlsx\", index_col=0)\n",
    "df_feats = pd.read_excel(f\"{path}/feats.xlsx\", index_col=0)\n",
    "feat_trgt = 'Возраст'\n",
    "feats_cnt = df_feats.index.to_list()\n",
    "feats_cat = []\n",
    "feats = list(feats_cnt) + list(feats_cat)\n",
    "feats_with_trgt = [feat_trgt] + feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check duplicated indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.index.duplicated(), [feat_trgt]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regenerate plots (if nesessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = 'crimson'\n",
    "n_cols = 4\n",
    "\n",
    "# Missing values matrix\n",
    "df_msno = data[feats + [feat_trgt]].copy()\n",
    "df_msno.sort_values([feat_trgt], ascending=[True], inplace=True)\n",
    "msno_mtx = msno.matrix(\n",
    "    df=df_msno,\n",
    "    label_rotation=90,\n",
    "    color=mcolors.to_rgb(color),\n",
    "    figsize=(0.7 * len(feats), 5),\n",
    ")\n",
    "plt.xticks(ha='center')\n",
    "plt.setp(msno_mtx.xaxis.get_majorticklabels(), ha=\"center\")\n",
    "msno_mtx.set_title(feats_title, fontsize='large')\n",
    "msno_mtx.set_ylabel(\"IDs\", fontsize='large')\n",
    "plt.savefig(f\"{path}/msno.png\", bbox_inches='tight', dpi=200)\n",
    "plt.savefig(f\"{path}/msno.pdf\", bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "# Age histogramm\n",
    "hist_bins = np.linspace(5, 115, 23)\n",
    "sns.set_theme(style='ticks')\n",
    "fig, ax = plt.subplots(figsize=(6, 3.5), layout='constrained')\n",
    "histplot = sns.histplot(\n",
    "    data=data,\n",
    "    bins=hist_bins,\n",
    "    edgecolor='k',\n",
    "    linewidth=1,\n",
    "    x=feat_trgt,\n",
    "    color=color,\n",
    "    ax=ax\n",
    ")\n",
    "histplot.set(xlim=(0, 120))\n",
    "histplot.set_ylabel('Количество')\n",
    "histplot.set_title(feats_title)\n",
    "plt.savefig(f\"{path}/age_hist.png\", bbox_inches='tight', dpi=200)\n",
    "plt.savefig(f\"{path}/age_hist.pdf\", bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Input features and output feature correlations\n",
    "df_corr = pd.DataFrame(index=feats, columns=['rho'])\n",
    "for f in tqdm(feats):\n",
    "    df_tmp = data.loc[:, [feat_trgt, f]].dropna(axis=0, how='any')\n",
    "    if df_tmp.shape[0] > 1:\n",
    "        vals_1 = df_tmp.loc[:, feat_trgt].values\n",
    "        vals_2 = df_tmp.loc[:, f].values\n",
    "        df_corr.at[f, 'rho'], _ = stats.pearsonr(vals_1, vals_2)\n",
    "df_corr.dropna(axis=0, how='any', inplace=True)\n",
    "df_corr.insert(1, \"abs(rho)\", df_corr['rho'].abs())\n",
    "df_corr.sort_values([\"abs(rho)\"], ascending=[False], inplace=True)\n",
    "df_corr = df_corr.apply(pd.to_numeric)\n",
    "sns.set_theme(style='ticks')\n",
    "fig, ax = plt.subplots(figsize=(0.9 + 0.039 * df_corr.index.str.len().max(), 0.9 + 0.4 * len(feats) + 0.04 * df_corr.index.str.len().max()) , layout='constrained')\n",
    "heatmap = sns.heatmap(\n",
    "    df_corr.loc[:, ['rho']],\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    vmin=-1.0,\n",
    "    vmax=1.0,\n",
    "    cmap='coolwarm',\n",
    "    linewidth=0.1,\n",
    "    linecolor='black',\n",
    "    #annot_kws={\"fontsize\": 15},\n",
    "    cbar_kws={\n",
    "        # \"shrink\": 0.9,\n",
    "        # \"aspect\": 30,\n",
    "        #'fraction': 0.046, \n",
    "        #'pad': 0.04,\n",
    "    },\n",
    "    ax=ax\n",
    ")\n",
    "heatmap_pos = ax.get_position()\n",
    "ax.figure.axes[-1].set_position([heatmap_pos.x1 + 0.05, heatmap_pos.y0, 0.1, heatmap_pos.height])\n",
    "ax.figure.axes[-1].set_ylabel(r\"Pearson $\\rho$\")\n",
    "for spine in ax.figure.axes[-1].spines.values():\n",
    "    spine.set(visible=True, lw=0.25, edgecolor=\"black\")\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.set_title(feats_title, fontsize=16)\n",
    "ax.set(xticklabels=[])\n",
    "ax.set(xticks=[])\n",
    "plt.savefig(f\"{path}/age_feats_pearsonr.png\", bbox_inches='tight', dpi=200)\n",
    "plt.savefig(f\"{path}/age_feats_pearsonr.pdf\", bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "\n",
    "n_rows = int(np.ceil(len(feats) / n_cols))\n",
    "n_empty = n_rows * n_cols - len(feats)\n",
    "sns.set_theme(style='ticks')\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=n_rows,\n",
    "    ncols=n_cols,\n",
    "    figsize=(n_cols * 3.0, n_rows * 2.5),\n",
    "    gridspec_kw={'wspace':0.10, 'hspace': 0.05}, \n",
    "    sharex=True,\n",
    "    layout='constrained'\n",
    ")\n",
    "for feat_id, feat in enumerate(df_corr.index.values):\n",
    "    row_id, col_id = divmod(feat_id, n_cols)\n",
    "    regplot = sns.regplot(\n",
    "        data=data,\n",
    "        x=feat_trgt,\n",
    "        y=feat,\n",
    "        color=color,\n",
    "        scatter_kws=dict(\n",
    "            linewidth=0.5,\n",
    "            alpha=0.75,\n",
    "            edgecolor=\"k\",\n",
    "            s=16,\n",
    "        ),\n",
    "        ax=axs[row_id, col_id]\n",
    "    )\n",
    "    axs[row_id, col_id].set_title(fr\"Pearson $\\rho$: {df_corr.loc[feat, 'rho']:0.3f}\")\n",
    "    y_labe_fontsize = min(15 / (len(feat) / 20), 13)\n",
    "    axs[row_id, col_id].set_ylabel(feat, fontsize=y_labe_fontsize)\n",
    "for empty_id in range(n_empty):   \n",
    "    axs[n_rows - 1, n_cols - 1 - empty_id].axis('off')\n",
    "fig.savefig(f\"{path}/age_feats_regplot.png\", bbox_inches='tight', dpi=200)\n",
    "fig.savefig(f\"{path}/age_feats_regplot.pdf\", bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "# Correlation heatmap\n",
    "df_corr = pd.DataFrame(data=np.zeros(shape=(len(feats_with_trgt), len(feats_with_trgt))), index=feats_with_trgt, columns=feats_with_trgt)\n",
    "for f_id_1 in range(len(feats_with_trgt)):\n",
    "    for f_id_2 in range(f_id_1, len(feats_with_trgt)):\n",
    "        f_1 = feats_with_trgt[f_id_1]\n",
    "        f_2 = feats_with_trgt[f_id_2]\n",
    "        if f_id_1 != f_id_2:\n",
    "            vals_1 = data.loc[:, f_1].values\n",
    "            vals_2 = data.loc[:, f_2].values\n",
    "            corr, pval = stats.pearsonr(vals_1, vals_2)\n",
    "            df_corr.at[f_2, f_1] = pval\n",
    "            df_corr.at[f_1, f_2] = corr\n",
    "        else:\n",
    "            df_corr.at[f_2, f_1] = np.nan\n",
    "selection = np.tri(df_corr.shape[0], df_corr.shape[1], -1, dtype=bool)\n",
    "df_fdr = df_corr.where(selection).stack().reset_index()\n",
    "df_fdr.columns = ['row', 'col', 'pval']\n",
    "_, df_fdr['pval_fdr_bh'], _, _ = multipletests(df_fdr.loc[:, 'pval'].values, 0.05, method='fdr_bh')\n",
    "nzmin = df_fdr['pval_fdr_bh'][df_fdr['pval_fdr_bh'].gt(0)].min(0) * 0.5\n",
    "df_fdr['pval_fdr_bh'].replace({0.0: nzmin}, inplace=True)\n",
    "df_corr_fdr = df_corr.copy()\n",
    "for line_id in range(df_fdr.shape[0]):\n",
    "    df_corr_fdr.loc[df_fdr.at[line_id, 'row'], df_fdr.at[line_id, 'col']] = -np.log10(df_fdr.at[line_id, 'pval_fdr_bh'])\n",
    "df_corr_fdr.to_excel(f\"{path}/feats_pearsonr.xlsx\")\n",
    "sns.set_theme(style='ticks')\n",
    "fig, ax = plt.subplots(figsize=(8.5 + 0.35 * len(feats_with_trgt), 6.5 + 0.25 * len(feats_with_trgt)), layout='constrained')\n",
    "cmap_triu = plt.get_cmap(\"seismic\").copy()\n",
    "mask_triu=np.tri(len(feats_with_trgt), len(feats_with_trgt), -1, dtype=bool)\n",
    "heatmap_diff = sns.heatmap(\n",
    "    df_corr_fdr,\n",
    "    mask=mask_triu,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    center=0.0,\n",
    "    cmap=cmap_triu,\n",
    "    linewidth=0.1,\n",
    "    linecolor='black',\n",
    "    annot_kws={\"fontsize\": 32 / np.sqrt(len(df_corr_fdr.values) + 8)},\n",
    "    ax=ax\n",
    ")\n",
    "ax.figure.axes[-1].set_ylabel(r\"Pearson $\\rho$\")\n",
    "for spine in ax.figure.axes[-1].spines.values():\n",
    "    spine.set(visible=True, lw=0.25, edgecolor=\"black\")\n",
    "cmap_tril = plt.get_cmap(\"viridis\").copy()\n",
    "cmap_tril.set_under('black')\n",
    "mask_tril=np.tri(len(feats_with_trgt), len(feats_with_trgt), -1, dtype=bool).T\n",
    "heatmap_pval = sns.heatmap(\n",
    "    df_corr_fdr,\n",
    "    mask=mask_tril,\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    "    vmin=-np.log10(0.05),\n",
    "    cmap=cmap_tril,\n",
    "    linewidth=0.1,\n",
    "    linecolor='black',\n",
    "    annot_kws={\"fontsize\": 32 / np.sqrt(len(df_corr_fdr.values) + 8)},\n",
    "    ax=ax\n",
    ")\n",
    "ax.figure.axes[-1].set_ylabel(r\"$-\\log_{10}(\\mathrm{p-value})$\")\n",
    "for spine in ax.figure.axes[-1].spines.values():\n",
    "    spine.set(visible=True, lw=0.25, edgecolor=\"black\")\n",
    "ax.set_xlabel('', fontsize=16)\n",
    "ax.set_ylabel('', fontsize=16)\n",
    "ax.set_title(feats_title, fontsize=16)\n",
    "plt.savefig(f\"{path}/feats_pearsonr.png\", bbox_inches='tight', dpi=200)\n",
    "plt.savefig(f\"{path}/feats_pearsonr.pdf\", bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "df_proc = data.copy()\n",
    "\n",
    "# IQR outliers\n",
    "out_columns = []\n",
    "for f in tqdm(feats_with_trgt):\n",
    "    q1 = df_proc[f].quantile(0.25)\n",
    "    q3 = df_proc[f].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    df_proc[f\"{f} IQR Outlier\"] = 1\n",
    "    out_columns.append(f\"{f} IQR Outlier\")\n",
    "    filter = (df_proc[f] >= q1 - 1.5 * iqr) & (df_proc[f] <= q3 + 1.5 * iqr)\n",
    "    df_proc.loc[filter, f\"{f} IQR Outlier\"] = 0\n",
    "df_proc[f\"Number of IQR Outliers\"] = df_proc.loc[:, out_columns].sum(axis=1)\n",
    "\n",
    "hist_bins = np.linspace(-0.5, len(feats_with_trgt) + 0.5, len(feats_with_trgt) + 2)\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "sns.set_theme(style='ticks')\n",
    "histplot = sns.histplot(\n",
    "    data=df_proc,\n",
    "    x=f\"Number of IQR Outliers\",\n",
    "    multiple=\"stack\",\n",
    "    bins=hist_bins,\n",
    "    edgecolor='k',\n",
    "    linewidth=1.0,\n",
    "    color=color,\n",
    ")\n",
    "histplot.set(xlim=(-0.5, max(df_proc['Number of IQR Outliers'] + 0.5)))\n",
    "histplot.set_title(feats_title)\n",
    "histplot.set_xlabel(\"Количество IQR выбросов\")\n",
    "histplot.set_ylabel(\"Количество записей\")\n",
    "plt.savefig(f\"{path}/outs_iqr_hist.png\", bbox_inches='tight', dpi=200)\n",
    "plt.savefig(f\"{path}/outs_iqr_hist.pdf\", bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "\n",
    "out_columns = [f\"{f} IQR Outlier\" for f in feats_with_trgt]\n",
    "df_msno = df_proc.loc[:, out_columns].copy()\n",
    "df_msno.replace({1: np.nan}, inplace=True)\n",
    "df_msno.rename(columns=dict(zip(out_columns, feats_with_trgt)), inplace=True)\n",
    "\n",
    "# Plot barplot for features with outliers\n",
    "msno_bar = msno.bar(\n",
    "    df=df_msno,\n",
    "    label_rotation=90,\n",
    "    color=color,\n",
    ")\n",
    "plt.xticks(ha='center')\n",
    "plt.setp(msno_bar.xaxis.get_majorticklabels(), ha=\"center\")\n",
    "msno_bar.set_title(feats_title, fontsize='large')\n",
    "msno_bar.set_ylabel(\"Записи без выбросов\", fontsize='large')\n",
    "plt.savefig(f\"{path}/outs_iqr_bar.png\", bbox_inches='tight', dpi=200)\n",
    "plt.savefig(f\"{path}/outs_iqr_bar.pdf\", bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "# Plot matrix of samples outliers distribution\n",
    "msno_mtx = msno.matrix(\n",
    "    df=df_msno,\n",
    "    label_rotation=90,\n",
    "    color=mcolors.to_rgb(color),\n",
    ")\n",
    "plt.xticks(ha='center')\n",
    "plt.setp(msno_bar.xaxis.get_majorticklabels(), ha=\"center\")\n",
    "msno_mtx.set_title(feats_title, fontsize='large')\n",
    "msno_mtx.set_ylabel(\"Записи\", fontsize='large')\n",
    "plt.savefig(f\"{path}/outs_iqr_matrix.png\", bbox_inches='tight', dpi=200)\n",
    "plt.savefig(f\"{path}/outs_iqr_matrix.pdf\", bbox_inches='tight')\n",
    "plt.clf()\n",
    "\n",
    "# Plot heatmap of features outliers correlations\n",
    "msno_heatmap = msno.heatmap(\n",
    "    df=df_msno,\n",
    "    label_rotation=90,\n",
    "    cmap=\"bwr\",\n",
    "    fontsize=12,\n",
    ")\n",
    "msno_heatmap.set_title(feats_title, fontsize='large')\n",
    "plt.setp(msno_heatmap.xaxis.get_majorticklabels(), ha=\"center\")\n",
    "msno_heatmap.collections[0].colorbar.ax.tick_params(labelsize=20)\n",
    "plt.savefig(f\"{path}/outs_iqr_heatmap.png\", bbox_inches='tight', dpi=200)\n",
    "plt.savefig(f\"{path}/outs_iqr_heatmap.pdf\", bbox_inches='tight')\n",
    "plt.clf()\n",
    "    \n",
    "# Dimensionality reduction\n",
    "dim_red_models = {\n",
    "    't-SNE': TSNE(n_components=2),\n",
    "    'PCA': PCA(n_components=2, whiten=False),\n",
    "    'IsoMap': Isomap(n_components=2, n_neighbors=5),\n",
    "    'MDS': MDS(n_components=2, metric=True),\n",
    "    'GRP': GaussianRandomProjection(n_components=2, eps=0.5),\n",
    "    'SRP': SparseRandomProjection(n_components=2, density='auto', eps=0.5, dense_output=False),\n",
    "}\n",
    "feats_dim_red = []\n",
    "for drm in dim_red_models:\n",
    "    dim_red_res = dim_red_models[drm].fit_transform(df_proc.loc[:, feats_with_trgt].values)\n",
    "    df_proc.loc[:, f\"{drm} 1\"] = dim_red_res[:, 0]\n",
    "    df_proc.loc[:, f\"{drm} 2\"] = dim_red_res[:, 1]\n",
    "    df_proc.loc[:, f\"{drm} HDBSCAN\"] = HDBSCAN(min_cluster_size=int(df_proc.shape[0] * 0.2)).fit(df_proc.loc[:, [f\"{drm} 1\", f\"{drm} 2\"]].values).labels_\n",
    "    feats_dim_red += [ f\"{drm} 1\",  f\"{drm} 2\"]\n",
    "n_rows = 2\n",
    "n_cols = 3\n",
    "fig_height = 10\n",
    "fig_width = 15\n",
    "sns.set_theme(style='ticks')\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(fig_width, fig_height), gridspec_kw={}, sharey=False, sharex=False, layout='constrained')\n",
    "for drm_id, drm in enumerate(dim_red_models.keys()):\n",
    "    row_id, col_id = divmod(drm_id, n_cols)\n",
    "    scatter = sns.scatterplot(\n",
    "        data=df_proc,\n",
    "        x=f\"{drm} 1\",\n",
    "        y=f\"{drm} 2\",\n",
    "        # hue=f\"{drm} HDBSCAN\",\n",
    "        hue='Пол',\n",
    "        palette={'М': 'deepskyblue', 'Ж': 'hotpink'},\n",
    "        linewidth=0.25,\n",
    "        alpha=0.75,\n",
    "        edgecolor=\"k\",\n",
    "        s=40,\n",
    "        ax=axs[row_id, col_id],\n",
    "    )\n",
    "    axs[row_id, col_id].set_title(drm)\n",
    "    # axs[n_rows - 1, n_cols - 1].axis('off')\n",
    "fig.suptitle(feats_title, fontsize='large')   \n",
    "fig.savefig(f\"{path}/dim_red.png\", bbox_inches='tight', dpi=200)\n",
    "fig.savefig(f\"{path}/dim_red.pdf\", bbox_inches='tight')\n",
    "df_proc.to_excel(f\"{path}/df_proc.xlsx\", index_label=\"ID\")\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "quantiles_all = pd.qcut(data[feat_trgt].values, quantiles, labels=False, duplicates='drop')\n",
    "unique_all, counts_all = np.unique(quantiles_all, return_counts=True)\n",
    "ids_all = data.index.values\n",
    "\n",
    "k_fold_all = RepeatedStratifiedKFold(\n",
    "    n_splits=tst_n_splits,\n",
    "    n_repeats=tst_n_repeats,\n",
    "    random_state=tst_random_state\n",
    ")\n",
    "splits_all = k_fold_all.split(X=ids_all, y=quantiles_all, groups=quantiles_all)\n",
    "for split_id, (ids_trn_val, ids_tst) in enumerate(splits_all):\n",
    "    data.loc[ids_all[ids_trn_val], f\"Split_{split_id}\"] = \"trn_val\"\n",
    "    data.loc[ids_all[ids_tst], f\"Split_{split_id}\"] = \"tst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        data[f\"Split_{split_id}_Fold_{fold_id}\"] = data[f\"Split_{split_id}\"]\n",
    "    \n",
    "    samples[split_id] = {\n",
    "        'test': data.index[data[f\"Split_{split_id}\"] == \"tst\"].values,\n",
    "        'train_validation': data.index[data[f\"Split_{split_id}\"] == \"trn_val\"].values,\n",
    "        'trains': {},\n",
    "        'validations': {},\n",
    "    }\n",
    "    \n",
    "    ids_trnval = data.index[(data[f\"Split_{split_id}\"] == 'trn_val')].values\n",
    "    \n",
    "    quantiles_trnval = pd.qcut(data.loc[ids_trnval, feat_trgt].values, quantiles, labels=False, duplicates='drop')\n",
    "    unique_trnval, counts_trnval = np.unique(quantiles_trnval, return_counts=True)\n",
    "    k_fold_trnval = RepeatedStratifiedKFold(\n",
    "        n_splits=val_n_splits,\n",
    "        n_repeats=val_n_repeats,\n",
    "        random_state=val_random_state\n",
    "    )\n",
    "    splits_trnval = k_fold_trnval.split(X=ids_trnval, y=quantiles_trnval, groups=quantiles_trnval)\n",
    "    for fold_id, (ids_trn, ids_val) in enumerate(splits_trnval):\n",
    "        data.loc[ids_trnval[ids_trn], f\"Split_{split_id}_Fold_{fold_id}\"] = \"trn\"\n",
    "        data.loc[ids_trnval[ids_val], f\"Split_{split_id}_Fold_{fold_id}\"] = \"val\"\n",
    "         \n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        samples[split_id]['trains'][fold_id] = data.index[data[f\"Split_{split_id}_Fold_{fold_id}\"] == \"trn\"].values\n",
    "        samples[split_id]['validations'][fold_id] = data.index[data[f\"Split_{split_id}_Fold_{fold_id}\"] == \"val\"].values\n",
    "\n",
    "    samples[split_id]['cv_indexes'] = [\n",
    "        (\n",
    "            np.where(data.index[data[f\"Split_{split_id}\"] == \"trn_val\"].isin(data.index[(data[f\"Split_{split_id}\"] == \"trn_val\") & (data[f\"Split_{split_id}_Fold_{i}\"] == 'trn')]))[0],\n",
    "            np.where(data.index[data[f\"Split_{split_id}\"] == \"trn_val\"].isin(data.index[(data[f\"Split_{split_id}\"] == \"trn_val\") & (data[f\"Split_{split_id}_Fold_{i}\"] == 'val')]))[0],\n",
    "        )\n",
    "        for i in range(val_n_splits * val_n_repeats)\n",
    "    ]\n",
    "    \n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        test_samples = samples[split_id]['test']\n",
    "        train_samples = samples[split_id]['trains'][fold_id]\n",
    "        validation_samples = samples[split_id]['validations'][fold_id]\n",
    "\n",
    "        intxns = {\n",
    "            'train_validation': set.intersection(set(train_samples), set(validation_samples)),\n",
    "            'validation_test': set.intersection(set(validation_samples), set(test_samples)),\n",
    "            'train_test': set.intersection(set(train_samples), set(test_samples))\n",
    "        }\n",
    "        \n",
    "        for intxn_name, intxn_samples in intxns.items():\n",
    "            if len(intxn_samples) > 0:\n",
    "                print(f\"Non-zero {intxn_name} intersection ({len(intxn_samples)}) for {split_id} Split and {fold_id} Fold!\")\n",
    "\n",
    "with open(f\"{path}/samples_tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_val({val_random_state}_{val_n_splits}_{val_n_repeats}).pickle\", 'wb') as handle:\n",
    "    pickle.dump(samples, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{path}/samples_tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_val({val_random_state}_{val_n_splits}_{val_n_repeats}).pickle\", 'rb') as handle:\n",
    "    samples = pickle.load(handle)\n",
    "    \n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        test_samples = samples[split_id]['test']\n",
    "        train_samples = samples[split_id]['trains'][fold_id]\n",
    "        validation_samples = samples[split_id]['validations'][fold_id]\n",
    "\n",
    "        intxns = {\n",
    "            'train_validation': set.intersection(set(train_samples), set(validation_samples)),\n",
    "            'validation_test': set.intersection(set(validation_samples), set(test_samples)),\n",
    "            'train_test': set.intersection(set(train_samples), set(test_samples))\n",
    "        }\n",
    "        \n",
    "        for intxn_name, intxn_samples in intxns.items():\n",
    "            if len(intxn_samples) > 0:\n",
    "                print(f\"Non-zero {intxn_name} intersection ({len(intxn_samples)}) for {split_id} Split and {fold_id} Fold!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tabular Model Sweep Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load non-model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_configs = \"E:/Git/bbs/notebooks/millennium/configs\"\n",
    "\n",
    "data_config = read_parse_config(f\"{path_configs}/DataConfig.yaml\", DataConfig)\n",
    "data_config['target'] = [feat_trgt]\n",
    "data_config['continuous_cols'] = feats_cnt\n",
    "data_config['categorical_cols'] = feats_cat\n",
    "trainer_config = read_parse_config(f\"{path_configs}/TrainerConfig.yaml\", TrainerConfig)\n",
    "pathlib.Path(path_models).mkdir(parents=True, exist_ok=True)\n",
    "trainer_config['checkpoints_path'] = path_models\n",
    "optimizer_config = read_parse_config(f\"{path_configs}/OptimizerConfig.yaml\", OptimizerConfig)\n",
    "\n",
    "lr_find_min_lr = 1e-8\n",
    "lr_find_max_lr = 1\n",
    "lr_find_num_training = 128\n",
    "lr_find_mode = \"exponential\"\n",
    "lr_find_early_stop_threshold = 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Search Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANDALF Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"model_config__gflu_stages\": [6],\n",
    "    \"model_config__gflu_dropout\": [0.1],\n",
    "    \"model_config__gflu_feature_init_sparsity\": [0.3],\n",
    "    \"model_config.head_config__dropout\": [0.1],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [1899],\n",
    "}\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)\n",
    "\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\",\n",
    "    activation='ReLU',\n",
    "    dropout=0.1,\n",
    "    use_batch_norm=False,\n",
    "    initialization=\"kaiming\"\n",
    ").__dict__\n",
    "\n",
    "model_list = []\n",
    "for i, params in enumerate(ParameterGrid(search_space)):\n",
    "    head_config_tmp = copy.deepcopy(head_config)\n",
    "    head_config_tmp['dropout'] = params['model_config.head_config__dropout']\n",
    "    model_config = read_parse_config(f\"{path_configs}/models/GANDALFConfig.yaml\", GANDALFConfig)\n",
    "    model_config['gflu_stages'] = params['model_config__gflu_stages']\n",
    "    model_config['gflu_feature_init_sparsity'] = params['model_config__gflu_feature_init_sparsity']\n",
    "    model_config['gflu_dropout'] = params['model_config__gflu_dropout']\n",
    "    model_config['learning_rate'] = params['model_config__learning_rate']\n",
    "    model_config['seed'] = params['model_config__seed']\n",
    "    model_config['head_config'] = head_config_tmp\n",
    "    model_list.append(GANDALFConfig(**model_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform model sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(f\"E:/Git/bbs/notebooks/millennium/pt/{feats_set}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "common_params = {\n",
    "    \"task\": \"regression\",\n",
    "}\n",
    "\n",
    "seed = 1899\n",
    "\n",
    "dfs_result = []\n",
    "for split_id, split_dict in samples.items():\n",
    "    for fold_id in split_dict['trains']:\n",
    "        test = data.loc[split_dict['test'], feats + [feat_trgt]]\n",
    "        train = data.loc[split_dict['trains'][fold_id], feats + [feat_trgt]]\n",
    "        validation = data.loc[split_dict['validations'][fold_id], feats + [feat_trgt]]\n",
    "\n",
    "        trainer_config['seed'] = seed\n",
    "        trainer_config['checkpoints'] = 'valid_loss'\n",
    "        trainer_config['load_best'] = True\n",
    "        trainer_config['auto_lr_find'] = True\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            sweep_df, best_model = model_sweep_custom(\n",
    "                task=\"regression\",\n",
    "                train=train,\n",
    "                validation=validation,\n",
    "                test=test,\n",
    "                data_config=data_config,\n",
    "                optimizer_config=optimizer_config,\n",
    "                trainer_config=trainer_config,\n",
    "                model_list=model_list,\n",
    "                common_model_args=common_params,\n",
    "                metrics=[\"mean_absolute_error\", \"pearson_corrcoef\"],\n",
    "                metrics_params=[{}, {}],\n",
    "                metrics_prob_input=[False, False],\n",
    "                rank_metric=(\"mean_absolute_error\", \"lower_is_better\"),\n",
    "                return_best_model=True,\n",
    "                seed=seed,\n",
    "                progress_bar=False,\n",
    "                verbose=False,\n",
    "                suppress_lightning_logger=True,\n",
    "                min_lr = lr_find_min_lr,\n",
    "                max_lr = lr_find_max_lr,\n",
    "                num_training = lr_find_num_training,\n",
    "                mode = lr_find_mode,\n",
    "                early_stop_threshold = lr_find_early_stop_threshold,\n",
    "            )\n",
    "        sweep_df['seed'] = seed\n",
    "        sweep_df['split_id'] = split_id\n",
    "        sweep_df['fold_id'] = fold_id\n",
    "        sweep_df[\"train_more\"] = False\n",
    "        sweep_df.loc[(sweep_df[\"train_loss\"] > sweep_df[\"test_loss\"]) | (sweep_df[\"train_loss\"] > sweep_df[\"validation_loss\"]), \"train_more\"] = True\n",
    "        sweep_df[\"validation_test_mean_loss\"] = (sweep_df[\"validation_loss\"] + sweep_df[\"test_loss\"]) / 2.0\n",
    "        sweep_df[\"train_validation_test_mean_loss\"] = (sweep_df[\"train_loss\"] + sweep_df[\"validation_loss\"] + sweep_df[\"test_loss\"]) / 3.0\n",
    "        \n",
    "        dfs_result.append(sweep_df)\n",
    "        \n",
    "        fn_suffix = (f\"models({len(model_list)})_\"\n",
    "                     f\"tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_\"\n",
    "                     f\"val({val_random_state}_{val_n_splits}_{val_n_repeats})\")\n",
    "        try:\n",
    "            df_result = pd.concat(dfs_result, ignore_index=True)\n",
    "            df_result.sort_values(by=['test_loss'], ascending=[True], inplace=True)\n",
    "            df_result.style.background_gradient(cmap=\"RdYlGn_r\").to_excel(f\"{trainer_config['checkpoints_path']}/{fn_suffix}.xlsx\")\n",
    "        except PermissionError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best models analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 1\n",
    "\n",
    "explain_method = \"GradientShap\"\n",
    "explain_baselines = \"b|1000\"\n",
    "explain_n_feats_to_plot = 25\n",
    "\n",
    "fn_sweep = (\n",
    "    f\"models({n_models})_\"\n",
    "    f\"tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_val({val_random_state}_{val_n_splits}_{val_n_repeats})\"\n",
    ")\n",
    "\n",
    "df_sweeps = pd.read_excel(f\"{path_models}/{fn_sweep}.xlsx\", index_col=0)\n",
    "path_to_candidates = f\"{path_models}/candidates/{fn_sweep}\"\n",
    "pathlib.Path(path_to_candidates).mkdir(parents=True, exist_ok=True)\n",
    "df_sweeps.style.background_gradient(\n",
    "    subset=[\n",
    "        \"train_loss\",\n",
    "        \"validation_loss\",\n",
    "        \"test_loss\",\n",
    "        \"time_taken\",\n",
    "        \"time_taken_per_epoch\"\n",
    "    ], cmap=\"RdYlGn_r\"\n",
    ").to_excel(f\"{path_to_candidates}/sweep.xlsx\")\n",
    "\n",
    "models_ids = [\n",
    "296\n",
    "]\n",
    "\n",
    "df_sweeps.loc[models_ids, :].style.background_gradient(\n",
    "    subset=[\n",
    "        \"train_loss\",\n",
    "        \"validation_loss\",\n",
    "        \"test_loss\",\n",
    "        \"time_taken\",\n",
    "        \"time_taken_per_epoch\"\n",
    "    ], cmap=\"RdYlGn_r\"\n",
    ").to_excel(f\"{path_to_candidates}/selected.xlsx\")\n",
    "\n",
    "for model_id in models_ids:\n",
    "\n",
    "    split_id = df_sweeps.at[model_id, 'split_id']\n",
    "    fold_id = df_sweeps.at[model_id, 'fold_id']\n",
    "    split_dict = samples[split_id]\n",
    "\n",
    "    test = data.loc[split_dict['test'], feats + [feat_trgt]]\n",
    "    train = data.loc[split_dict['trains'][fold_id], feats + [feat_trgt]]\n",
    "    validation = data.loc[split_dict['validations'][fold_id], feats + [feat_trgt]]\n",
    "\n",
    "    model_dir = str(pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).parent).replace('\\\\', '/') + '/' + pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).stem\n",
    "    model = TabularModel.load_model(model_dir)\n",
    "    pathlib.Path(f\"{path_to_candidates}/{model_id}\").mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copytree(model_dir, f\"{path_to_candidates}/{model_id}\", dirs_exist_ok=True)\n",
    "    \n",
    "    df = data.loc[:, [feat_trgt]]\n",
    "    df.loc[train.index, 'Group'] = 'Train'\n",
    "    df.loc[validation.index, 'Group'] = 'Validation'\n",
    "    df.loc[test.index, 'Group'] = 'Test'\n",
    "    df['Prediction'] = model.predict(data)\n",
    "    df['Error'] = df['Prediction'] - df[feat_trgt]\n",
    "    corrector = LinearBiasCorrector()\n",
    "    corrector.fit(df.loc[df['Group'] == 'Train', feat_trgt].values, df.loc[df['Group'] == 'Train', 'Prediction'].values)\n",
    "    df['Prediction Unbiased'] = corrector.predict(df['Prediction'].values)\n",
    "    df['Error Unbiased'] = df['Prediction Unbiased'] - df[feat_trgt]\n",
    "    df.to_excel(f\"{path_to_candidates}/{model_id}/df.xlsx\")\n",
    "    \n",
    "    colors_groups = {\n",
    "        'Train': 'chartreuse',\n",
    "        'Validation': 'dodgerblue',\n",
    "        'Test': 'crimson',\n",
    "    }\n",
    "    \n",
    "    df_metrics = pd.DataFrame(\n",
    "        index=list(colors_groups.keys()),\n",
    "        columns=[\n",
    "            'mean_absolute_error', 'pearson_corrcoef', 'bias',\n",
    "            'mean_absolute_error_unbiased', 'pearson_corrcoef_unbiased', 'bias_unbiased'\n",
    "        ]\n",
    "    )\n",
    "    for group in colors_groups.keys():\n",
    "        pred = torch.from_numpy(df.loc[df['Group'] == group, 'Prediction'].values)\n",
    "        pred_unbiased = torch.from_numpy(df.loc[df['Group'] == group, 'Prediction Unbiased'].values)\n",
    "        real = torch.from_numpy(df.loc[df['Group'] == group, feat_trgt].values.astype(np.float32))\n",
    "        df_metrics.at[group, 'mean_absolute_error'] = mean_absolute_error(pred, real).numpy()\n",
    "        df_metrics.at[group, 'pearson_corrcoef'] = pearson_corrcoef(pred, real).numpy()\n",
    "        df_metrics.at[group, 'bias'] = np.mean(df.loc[df['Group'] == group, 'Error'].values)\n",
    "        df_metrics.at[group, 'mean_absolute_error_unbiased'] = mean_absolute_error(pred_unbiased, real).numpy()\n",
    "        df_metrics.at[group, 'pearson_corrcoef_unbiased'] = pearson_corrcoef(pred_unbiased, real).numpy()\n",
    "        df_metrics.at[group, 'bias_unbiased'] = np.mean(df.loc[df['Group'] == group, 'Error Unbiased'].values)\n",
    "    df_metrics.to_excel(f\"{path_to_candidates}/{model_id}/metrics.xlsx\", index_label=\"Metrics\")\n",
    "    \n",
    "    xy_min = df[[feat_trgt, 'Prediction']].min().min()\n",
    "    xy_max = df[[feat_trgt, 'Prediction']].max().max()\n",
    "    xy_ptp = xy_max - xy_min\n",
    "    \n",
    "    xy_min_unbiased = df[[feat_trgt, 'Prediction Unbiased']].min().min()\n",
    "    xy_max_unbiased = df[[feat_trgt, 'Prediction Unbiased']].max().max()\n",
    "    xy_ptp_unbiased = xy_max_unbiased - xy_min_unbiased\n",
    "    \n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "    for group in colors_groups.keys():    \n",
    "        regplot = sns.regplot(\n",
    "            data=df.loc[df['Group'] == group, :],\n",
    "            x=feat_trgt,\n",
    "            y=\"Prediction\",\n",
    "            label=group,\n",
    "            color=colors_groups[group],\n",
    "            scatter_kws=dict(\n",
    "                linewidth=0.2,\n",
    "                alpha=0.75,\n",
    "                edgecolor=\"k\",\n",
    "                s=20,\n",
    "            ),\n",
    "            ax=ax\n",
    "        )\n",
    "    bisect = sns.lineplot(\n",
    "        x=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        y=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        linestyle='--',\n",
    "        color='black',\n",
    "        linewidth=1.0,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params)\")\n",
    "    ax.set_xlim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    ax.set_ylim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "    for group in colors_groups.keys():    \n",
    "        regplot = sns.regplot(\n",
    "            data=df.loc[df['Group'] == group, :],\n",
    "            x=feat_trgt,\n",
    "            y=\"Prediction Unbiased\",\n",
    "            label=group,\n",
    "            color=colors_groups[group],\n",
    "            scatter_kws=dict(\n",
    "                linewidth=0.2,\n",
    "                alpha=0.75,\n",
    "                edgecolor=\"k\",\n",
    "                s=20,\n",
    "            ),\n",
    "            ax=ax\n",
    "        )\n",
    "    bisect = sns.lineplot(\n",
    "        x=[xy_min_unbiased - 0.1 * xy_ptp_unbiased, xy_max_unbiased + 0.1 * xy_ptp_unbiased],\n",
    "        y=[xy_min_unbiased - 0.1 * xy_ptp_unbiased, xy_max_unbiased + 0.1 * xy_ptp_unbiased],\n",
    "        linestyle='--',\n",
    "        color='black',\n",
    "        linewidth=1.0,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params)\")\n",
    "    ax.set_xlim(xy_min_unbiased - 0.1 * xy_ptp, xy_max_unbiased + 0.1 * xy_ptp_unbiased)\n",
    "    ax.set_ylim(xy_min_unbiased - 0.1 * xy_ptp, xy_max_unbiased + 0.1 * xy_ptp_unbiased)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot_unbiased.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/regplot_unbiased.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))   \n",
    "    scatter = sns.scatterplot(\n",
    "        data=df,\n",
    "        x=feat_trgt,\n",
    "        y=\"Prediction\",\n",
    "        hue=\"Group\",\n",
    "        palette=colors_groups,\n",
    "        linewidth=0.2,\n",
    "        alpha=0.75,\n",
    "        edgecolor=\"k\",\n",
    "        s=20,\n",
    "        hue_order=list(colors_groups.keys()),\n",
    "        ax=ax\n",
    "    )\n",
    "    bisect = sns.lineplot(\n",
    "        x=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        y=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        linestyle='--',\n",
    "        color='black',\n",
    "        linewidth=1.0,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params)\")\n",
    "    ax.set_xlim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    ax.set_ylim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/scatter.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/scatter.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    df_fig = df.loc[:, ['Error', 'Group']]\n",
    "    groups_rename = {\n",
    "        group: f\"{group}\" + \"\\n\" +\n",
    "               fr\"MAE: {df_metrics.at[group, 'mean_absolute_error']:0.2f}\" + \"\\n\"\n",
    "               fr\"Pearson $\\rho$: {df_metrics.at[group, 'pearson_corrcoef']:0.2f}\" + \"\\n\" +\n",
    "               fr\"$\\langle$Error$\\rangle$: {df_metrics.at[group, 'bias']:0.2f}\" \n",
    "        for group in colors_groups\n",
    "    }\n",
    "    colors_groups_violin = {groups_rename[group]: colors_groups[group] for group in colors_groups}\n",
    "    df_fig['Group'].replace(groups_rename, inplace=True)\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    violin = sns.violinplot(\n",
    "        data=df_fig,\n",
    "        x='Group',\n",
    "        y='Error',\n",
    "        palette=colors_groups_violin,\n",
    "        scale='width',\n",
    "        order=list(colors_groups_violin.keys()),\n",
    "        saturation=0.75,\n",
    "        legend=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel('')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    df_fig = df.loc[:, ['Error Unbiased', 'Group']]\n",
    "    groups_rename = {\n",
    "        group: f\"{group}\" + \"\\n\" +\n",
    "               fr\"MAE: {df_metrics.at[group, 'mean_absolute_error_unbiased']:0.2f}\" + \"\\n\"\n",
    "               fr\"Pearson $\\rho$: {df_metrics.at[group, 'pearson_corrcoef_unbiased']:0.2f}\" + \"\\n\" +\n",
    "               fr\"$\\langle$Error$\\rangle$: {df_metrics.at[group, 'bias_unbiased']:0.2f}\" \n",
    "        for group in colors_groups\n",
    "    }\n",
    "    colors_groups_violin = {groups_rename[group]: colors_groups[group] for group in colors_groups}\n",
    "    df_fig['Group'].replace(groups_rename, inplace=True)\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    violin = sns.violinplot(\n",
    "        data=df_fig,\n",
    "        x='Group',\n",
    "        y='Error Unbiased',\n",
    "        palette=colors_groups_violin,\n",
    "        scale='width',\n",
    "        order=list(colors_groups_violin.keys()),\n",
    "        saturation=0.75,\n",
    "        legend=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel('')\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin_unbiased.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_to_candidates}/{model_id}/violin_unbiased.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    try:\n",
    "        explanation = model.explain(data, method=explain_method, baselines=explain_baselines)\n",
    "        explanation.index = data.index\n",
    "        explanation.to_excel(f\"{path_to_candidates}/{model_id}/explanation.xlsx\")\n",
    "        \n",
    "        sns.set_theme(style='whitegrid')\n",
    "        fig = shap.summary_plot(\n",
    "            shap_values=explanation.loc[:, feats].values,\n",
    "            features=data.loc[:, feats].values,\n",
    "            feature_names=feats,\n",
    "            max_display=explain_n_feats_to_plot,\n",
    "            plot_type=\"violin\",\n",
    "            show=False,\n",
    "        )\n",
    "        plt.savefig(f\"{path_to_candidates}/{model_id}/explain_beeswarm.png\", bbox_inches='tight', dpi=200)\n",
    "        plt.savefig(f\"{path_to_candidates}/{model_id}/explain_beeswarm.pdf\", bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        sns.set_theme(style='ticks')\n",
    "        fig = shap.summary_plot(\n",
    "            shap_values=explanation.loc[:, feats].values,\n",
    "            features=data.loc[:, feats].values,\n",
    "            feature_names=feats,\n",
    "            max_display=explain_n_feats_to_plot,\n",
    "            plot_type=\"bar\",\n",
    "            show=False,\n",
    "            plot_size=[12,8]\n",
    "        )\n",
    "        plt.savefig(f\"{path_to_candidates}/{model_id}/explain_bar.png\", bbox_inches='tight', dpi=200)\n",
    "        plt.savefig(f\"{path_to_candidates}/{model_id}/explain_bar.pdf\", bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    \n",
    "    except NotImplementedError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
