{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Debugging autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T14:29:22.801727Z",
     "start_time": "2024-07-01T14:29:18.695223Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pytorch_tabular.utils import load_covertype_dataset\n",
    "from rich.pretty import pprint\n",
    "from sklearn.model_selection import BaseCrossValidator, ParameterGrid, ParameterSampler\n",
    "import torch\n",
    "import pickle\n",
    "import shutil\n",
    "import shap\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from glob import glob\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pytorch_tabular.utils import make_mixed_dataset, print_metrics\n",
    "from pytorch_tabular import available_models\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig, GANDALFConfig, TabNetModelConfig, FTTransformerConfig, DANetConfig\n",
    "from pytorch_tabular.config import DataConfig, OptimizerConfig, TrainerConfig\n",
    "from pytorch_tabular.models.common.heads import LinearHeadConfig\n",
    "from pytorch_tabular.tabular_model_tuner import TabularModelTuner\n",
    "from torchmetrics.functional.regression import mean_absolute_error, pearson_corrcoef\n",
    "from pytorch_tabular import MODEL_SWEEP_PRESETS\n",
    "import pandas as pd\n",
    "from pytorch_tabular import model_sweep\n",
    "from src.pt.model_sweep import model_sweep_custom\n",
    "import warnings\n",
    "from src.utils.configs import read_parse_config\n",
    "from src.utils.hash import dict_hash\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import distinctipy\n",
    "import matplotlib.patheffects as pe\n",
    "import matplotlib.colors as mcolors\n",
    "from statannotations.Annotator import Annotator\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "\n",
    "def make_rgb_transparent(rgb, bg_rgb, alpha):\n",
    "    return [alpha * c1 + (1 - alpha) * c2 for (c1, c2) in zip(rgb, bg_rgb)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T14:29:45.850727Z",
     "start_time": "2024-07-01T14:29:22.801727Z"
    }
   },
   "outputs": [],
   "source": [
    "epi_data_type = 'no_harm'\n",
    "imm_data_type = 'imp_source(imm)_method(knn)_params(5)' # 'origin' 'imp_source(imm)_method(knn)_params(5)' 'imp_source(imm)_method(miceforest)_params(2)'\n",
    "\n",
    "selection_method = 'mrmr' # 'f_regression' 'mrmr'\n",
    "n_feats = 100\n",
    "\n",
    "tst_n_splits = 5\n",
    "tst_n_repeats = 5\n",
    "tst_random_state = 1337\n",
    "\n",
    "val_n_splits = 4\n",
    "val_n_repeats = 4\n",
    "val_random_state = 1337\n",
    "\n",
    "path = f\"D:/YandexDisk/Work/bbd/immunology/003_EpImAge/{imm_data_type}/{epi_data_type}/{selection_method}_{n_feats}/EpImAge\"\n",
    "path_epi = f\"D:/YandexDisk/Work/bbd/immunology/003_EpImAge/epi\"\n",
    "\n",
    "data_full = pd.read_excel(f\"{path}/data.xlsx\", index_col=0)\n",
    "\n",
    "# Filtering data\n",
    "status_count = data_full['Status'].value_counts()\n",
    "statuses_passed = status_count[status_count >= 10].index.values.tolist()\n",
    "data_full = data_full[data_full['Status'].isin(statuses_passed)]\n",
    "data_full.drop(data_full.index[data_full['Status'] == 'ICU'], inplace=True)\n",
    "data_full.to_excel(f\"{path}/data_filtered.xlsx\", index_label='ID')\n",
    "\n",
    "status_count = data_full['Status'].value_counts()\n",
    "\n",
    "data = data_full[data_full['Status'] == 'Control']\n",
    "\n",
    "feats = pd.read_excel(f\"{path}/feats.xlsx\", index_col=0).index.values.tolist()\n",
    "feats = [f\"{f}_log\" for f in feats]\n",
    "\n",
    "gse_preproc = pd.read_excel(f\"{path_epi}/preproc.xlsx\", index_col=0)\n",
    "\n",
    "df_groups = pd.read_excel(f\"{path_epi}/groups.xlsx\", index_col=0)\n",
    "icd_chpts = np.sort(df_groups['ICD-11 chapter'].unique())\n",
    "icd_codes = np.sort(df_groups['ICD-11 code'].unique())\n",
    "colors = distinctipy.get_colors(len(icd_chpts), [mcolors.hex2color(mcolors.CSS4_COLORS['black']), mcolors.hex2color(mcolors.CSS4_COLORS['white'])], rng=1337, pastel_factor=0.5)\n",
    "colors = [make_rgb_transparent(color, (1, 1, 1), 0.75) for color in colors]\n",
    "colors_icd_chpts = {icd_chpt: colors[icd_chpt_id] for icd_chpt_id, icd_chpt in enumerate(icd_chpts)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T14:29:46.387990Z",
     "start_time": "2024-07-01T14:29:45.850727Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"{path}/samples_tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_val({val_random_state}_{val_n_splits}_{val_n_repeats}).pickle\", 'rb') as handle:\n",
    "    samples = pickle.load(handle)\n",
    "    \n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        test_samples = samples[split_id]['test']\n",
    "        train_samples = samples[split_id]['trains'][fold_id]\n",
    "        validation_samples = samples[split_id]['validations'][fold_id]\n",
    "\n",
    "        intxns = {\n",
    "            'train_validation': set.intersection(set(train_samples), set(validation_samples)),\n",
    "            'validation_test': set.intersection(set(validation_samples), set(test_samples)),\n",
    "            'train_test': set.intersection(set(train_samples), set(test_samples))\n",
    "        }\n",
    "        \n",
    "        for intxn_name, intxn_samples in intxns.items():\n",
    "            if len(intxn_samples) > 0:\n",
    "                print(f\"Non-zero {intxn_name} intersection ({len(intxn_samples)}) for {split_id} Split and {fold_id} Fold!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "gse_count = data['GSE'].value_counts()\n",
    "stratify_cat_parts_all = {gse: data.index[data['GSE'] == gse].values for gse, count in gse_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part_all, ids_all in (pbar := tqdm(stratify_cat_parts_all.items())):\n",
    "    pbar.set_description(f\"Processing {part_all} ({len(ids_all)})\")\n",
    "    quantiles_all = pd.qcut(data.loc[ids_all, 'Age'].values, quantiles, labels=False, duplicates='drop')\n",
    "    unique_all, counts_all = np.unique(quantiles_all, return_counts=True)\n",
    "    \n",
    "    if max(counts_all) >= len(quantiles):\n",
    "        k_fold_all = RepeatedStratifiedKFold(\n",
    "            n_splits=tst_n_splits,\n",
    "            n_repeats=tst_n_repeats,\n",
    "            random_state=tst_random_state\n",
    "        )\n",
    "        splits_all = k_fold_all.split(X=ids_all, y=quantiles_all, groups=quantiles_all)\n",
    "        for split_id, (ids_trn_val, ids_tst) in enumerate(splits_all):\n",
    "            data.loc[ids_all[ids_trn_val], f\"Split_{split_id}\"] = \"trn_val\"\n",
    "            data.loc[ids_all[ids_tst], f\"Split_{split_id}\"] = \"tst\"\n",
    "            \n",
    "    else:\n",
    "        for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "            data.loc[ids_all, f\"Split_{split_id}\"] = \"tst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        data[f\"Split_{split_id}_Fold_{fold_id}\"] = data[f\"Split_{split_id}\"]\n",
    "    \n",
    "    samples[split_id] = {\n",
    "        'test': data.index[data[f\"Split_{split_id}\"] == \"tst\"].values,\n",
    "        'train_validation': data.index[data[f\"Split_{split_id}\"] == \"trn_val\"].values,\n",
    "        'trains': {},\n",
    "        'validations': {},\n",
    "    }\n",
    "    \n",
    "    stratify_cat_parts_trnval = {}\n",
    "    for gse, count in gse_count.items():\n",
    "        gse_ids = data.index[(data['GSE'] == gse) & (data[f\"Split_{split_id}\"] == 'trn_val')].values\n",
    "        if len(gse_ids) > 0:\n",
    "            stratify_cat_parts_trnval[gse] = gse_ids\n",
    "\n",
    "    for part_trnval, ids_trnval in stratify_cat_parts_trnval.items():\n",
    "        quantiles_trnval = pd.qcut(data.loc[ids_trnval, 'Age'].values, quantiles, labels=False, duplicates='drop')\n",
    "        unique_trnval, counts_trnval = np.unique(quantiles_trnval, return_counts=True)\n",
    "        k_fold_trnval = RepeatedStratifiedKFold(\n",
    "            n_splits=val_n_splits,\n",
    "            n_repeats=val_n_repeats,\n",
    "            random_state=val_random_state\n",
    "        )\n",
    "        splits_trnval = k_fold_trnval.split(X=ids_trnval, y=quantiles_trnval, groups=quantiles_trnval)\n",
    "        for fold_id, (ids_trn, ids_val) in enumerate(splits_trnval):\n",
    "            data.loc[ids_trnval[ids_trn], f\"Split_{split_id}_Fold_{fold_id}\"] = \"trn\"\n",
    "            data.loc[ids_trnval[ids_val], f\"Split_{split_id}_Fold_{fold_id}\"] = \"val\"\n",
    "         \n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        samples[split_id]['trains'][fold_id] = data.index[data[f\"Split_{split_id}_Fold_{fold_id}\"] == \"trn\"].values\n",
    "        samples[split_id]['validations'][fold_id] = data.index[data[f\"Split_{split_id}_Fold_{fold_id}\"] == \"val\"].values\n",
    "\n",
    "    samples[split_id]['cv_indexes'] = [\n",
    "        (\n",
    "            np.where(data.index[data[f\"Split_{split_id}\"] == \"trn_val\"].isin(data.index[(data[f\"Split_{split_id}\"] == \"trn_val\") & (data[f\"Split_{split_id}_Fold_{i}\"] == 'trn')]))[0],\n",
    "            np.where(data.index[data[f\"Split_{split_id}\"] == \"trn_val\"].isin(data.index[(data[f\"Split_{split_id}\"] == \"trn_val\") & (data[f\"Split_{split_id}_Fold_{i}\"] == 'val')]))[0],\n",
    "        )\n",
    "        for i in range(val_n_splits * val_n_repeats)\n",
    "    ]\n",
    "\n",
    "with open(f\"{path}/samples_tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_val({val_random_state}_{val_n_splits}_{val_n_repeats}).pickle\", 'wb') as handle:\n",
    "    pickle.dump(samples, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check samples intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T14:30:23.996181Z",
     "start_time": "2024-07-01T14:30:23.550027Z"
    }
   },
   "outputs": [],
   "source": [
    "for split_id in range(tst_n_splits * tst_n_repeats):\n",
    "    for fold_id in range(val_n_splits * val_n_repeats):\n",
    "        test_samples = samples[split_id]['test']\n",
    "        train_samples = samples[split_id]['trains'][fold_id]\n",
    "        validation_samples = samples[split_id]['validations'][fold_id]\n",
    "\n",
    "        intxns = {\n",
    "            'train_validation': set.intersection(set(train_samples), set(validation_samples)),\n",
    "            'validation_test': set.intersection(set(validation_samples), set(test_samples)),\n",
    "            'train_test': set.intersection(set(train_samples), set(test_samples))\n",
    "        }\n",
    "        \n",
    "        for intxn_name, intxn_samples in intxns.items():\n",
    "            if len(intxn_samples) > 0:\n",
    "                print(f\"Non-zero {intxn_name} intersection ({len(intxn_samples)}) for {split_id} Split and {fold_id} Fold!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sweep Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load non-model configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T14:32:15.453619Z",
     "start_time": "2024-07-01T14:32:15.439421Z"
    }
   },
   "outputs": [],
   "source": [
    "path_configs = \"D:/Work/bbs/notebooks/immunology/003_EpImAge/age_regression_configs\"\n",
    "\n",
    "data_config = read_parse_config(f\"{path_configs}/DataConfig.yaml\", DataConfig)\n",
    "data_config['target'] = ['Age']\n",
    "data_config['continuous_cols'] = feats\n",
    "trainer_config = read_parse_config(f\"{path_configs}/TrainerConfig.yaml\", TrainerConfig)\n",
    "trainer_config['checkpoints_path'] = f\"{path}/pytorch_tabular\"\n",
    "optimizer_config = read_parse_config(f\"{path_configs}/OptimizerConfig.yaml\", OptimizerConfig)\n",
    "\n",
    "lr_find_min_lr = 1e-8\n",
    "lr_find_max_lr = 1\n",
    "lr_find_num_training = 256\n",
    "lr_find_mode = \"exponential\"\n",
    "lr_find_early_stop_threshold = 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Models Search Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANDALF Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T14:32:17.511989Z",
     "start_time": "2024-07-01T14:32:17.498513Z"
    }
   },
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"model_config__gflu_stages\": [4, 6, 8],\n",
    "    \"model_config__gflu_dropout\": [0.1],\n",
    "    \"model_config__gflu_feature_init_sparsity\": [0.3],\n",
    "    \"model_config.head_config__dropout\": [0.1],\n",
    "    \"model_config__learning_rate\": [0.001],\n",
    "    \"model_config__seed\": [451, 1408],\n",
    "}\n",
    "grid_size = np.prod([len(p_vals) for _, p_vals in search_space.items()])\n",
    "print(grid_size)\n",
    "\n",
    "head_config = LinearHeadConfig(\n",
    "    layers=\"\",\n",
    "    activation='ReLU',\n",
    "    dropout=0.1,\n",
    "    use_batch_norm=False,\n",
    "    initialization=\"kaiming\"\n",
    ").__dict__\n",
    "\n",
    "model_list = []\n",
    "for i, params in enumerate(ParameterGrid(search_space)):\n",
    "    head_config_tmp = copy.deepcopy(head_config)\n",
    "    head_config_tmp['dropout'] = params['model_config.head_config__dropout']\n",
    "    model_config = read_parse_config(f\"{path_configs}/models/GANDALFConfig.yaml\", GANDALFConfig)\n",
    "    model_config['gflu_stages'] = params['model_config__gflu_stages']\n",
    "    model_config['gflu_feature_init_sparsity'] = params['model_config__gflu_feature_init_sparsity']\n",
    "    model_config['gflu_dropout'] = params['model_config__gflu_dropout']\n",
    "    model_config['learning_rate'] = params['model_config__learning_rate']\n",
    "    model_config['seed'] = params['model_config__seed']\n",
    "    model_config['head_config'] = head_config_tmp\n",
    "    model_list.append(GANDALFConfig(**model_config))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Perform model sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T01:26:46.422051Z",
     "start_time": "2024-07-01T14:32:24.479515Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "common_params = {\n",
    "    \"task\": \"regression\",\n",
    "}\n",
    "\n",
    "seed = 451\n",
    "\n",
    "dfs_result = []\n",
    "for split_id, split_dict in samples.items():\n",
    "    for fold_id in split_dict['trains']:\n",
    "        test = data.loc[split_dict['test'], feats + [\"Age\"]]\n",
    "        train = data.loc[split_dict['trains'][fold_id], feats + [\"Age\"]]\n",
    "        validation = data.loc[split_dict['validations'][fold_id], feats + [\"Age\"]]\n",
    "\n",
    "        trainer_config['seed'] = seed\n",
    "        trainer_config['checkpoints'] = 'valid_loss'\n",
    "        trainer_config['load_best'] = True\n",
    "        trainer_config['auto_lr_find'] = True\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            sweep_df, best_model = model_sweep_custom(\n",
    "                task=\"regression\",\n",
    "                train=train,\n",
    "                validation=validation,\n",
    "                test=test,\n",
    "                data_config=data_config,\n",
    "                optimizer_config=optimizer_config,\n",
    "                trainer_config=trainer_config,\n",
    "                model_list=model_list,\n",
    "                common_model_args=common_params,\n",
    "                metrics=[\"mean_absolute_error\", \"pearson_corrcoef\"],\n",
    "                metrics_params=[{}, {}],\n",
    "                metrics_prob_input=[False, False],\n",
    "                rank_metric=(\"mean_absolute_error\", \"lower_is_better\"),\n",
    "                return_best_model=True,\n",
    "                seed=seed,\n",
    "                progress_bar=False,\n",
    "                verbose=False,\n",
    "                suppress_lightning_logger=True,\n",
    "                min_lr = lr_find_min_lr,\n",
    "                max_lr = lr_find_max_lr,\n",
    "                num_training = lr_find_num_training,\n",
    "                mode = lr_find_mode,\n",
    "                early_stop_threshold = lr_find_early_stop_threshold,\n",
    "            )\n",
    "        sweep_df['seed'] = seed\n",
    "        sweep_df['split_id'] = split_id\n",
    "        sweep_df['fold_id'] = fold_id\n",
    "        sweep_df[\"train_more\"] = False\n",
    "        sweep_df.loc[(sweep_df[\"train_loss\"] > sweep_df[\"test_loss\"]) | (sweep_df[\"train_loss\"] > sweep_df[\"validation_loss\"]), \"train_more\"] = True\n",
    "        sweep_df[\"validation_test_mean_loss\"] = (sweep_df[\"validation_loss\"] + sweep_df[\"test_loss\"]) / 2.0\n",
    "        sweep_df[\"train_validation_test_mean_loss\"] = (sweep_df[\"train_loss\"] + sweep_df[\"validation_loss\"] + sweep_df[\"test_loss\"]) / 3.0\n",
    "        \n",
    "        dfs_result.append(sweep_df)\n",
    "        \n",
    "        fn_suffix = (f\"models({len(model_list)})_\"\n",
    "                     f\"tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_\"\n",
    "                     f\"val({val_random_state}_{val_n_splits}_{val_n_repeats})\")\n",
    "        try:\n",
    "            df_result = pd.concat(dfs_result, ignore_index=True)\n",
    "            df_result.sort_values(by=['test_loss'], ascending=[True], inplace=True)\n",
    "            df_result.style.background_gradient(\n",
    "                subset=[\n",
    "                    \"train_loss\",\n",
    "                    \"validation_loss\",\n",
    "                    \"test_loss\",\n",
    "                    \"time_taken\",\n",
    "                    \"time_taken_per_epoch\"\n",
    "                ], cmap=\"RdYlGn_r\"\n",
    "            ).to_excel(f\"{trainer_config['checkpoints_path']}/{fn_suffix}.xlsx\")\n",
    "        except PermissionError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Best models analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gse_count = data['GSE'].value_counts()\n",
    "gses = gse_count.index.values\n",
    "gse_ids = {gse: data.index[data['GSE'] == gse].values for gse, count in gse_count.items()}\n",
    "colors = distinctipy.get_colors(len(gses), [mcolors.hex2color(mcolors.CSS4_COLORS['white']), mcolors.hex2color(mcolors.CSS4_COLORS['black'])], rng=1337)\n",
    "colors_gse = {gses[gse_id]: colors[gse_id] for gse_id in range(len(gses))}\n",
    "statuses_rename = {x: x.replace(' ', '\\n').replace('-', '\\n') for x in data_full['Status'].value_counts().index.values}\n",
    "\n",
    "data_full['Status'] = data_full['Status'].replace(statuses_rename) \n",
    "status_count = data_full['Status'].value_counts()\n",
    "statuses = status_count.index.values\n",
    "colors = distinctipy.get_colors(len(statuses) - 1, [mcolors.hex2color(mcolors.CSS4_COLORS['dodgerblue']), mcolors.hex2color(mcolors.CSS4_COLORS['white']), mcolors.hex2color(mcolors.CSS4_COLORS['black'])], rng=1337)\n",
    "colors_status = {'Control': mcolors.hex2color(mcolors.CSS4_COLORS['dodgerblue'])}\n",
    "for status_id, status in enumerate(statuses):\n",
    "    if status != 'Control':\n",
    "        colors_status[status] = colors[status_id - 1]\n",
    "\n",
    "mosaic_violins = []\n",
    "mosaic_rows = np.sort(df_groups['Row on violin plot'].unique())\n",
    "for mosaic_row in mosaic_rows:\n",
    "    df_mosaic_row = df_groups[df_groups['Row on violin plot'] == mosaic_row].sort_values(by=['ICD-11 chapter', 'ICD-11 code', 'GSE'], ascending=[True, True, True])\n",
    "    mosaic_row_labels = []\n",
    "    for plot_id, plot_row in df_mosaic_row.iterrows():\n",
    "        n_violins = len(ast.literal_eval(plot_row['Statuses']))\n",
    "        mosaic_row_labels += [plot_id]*n_violins\n",
    "    mosaic_violins.append(mosaic_row_labels)\n",
    "    \n",
    "max_mosaic_row = max([len(x) for x in mosaic_violins])\n",
    "violons_empty_panels = set()\n",
    "for row_id, row in enumerate(mosaic_violins):\n",
    "    for added_spaces in range(len(row), max_mosaic_row):\n",
    "        violons_empty_panels.add(f'Empty row {row_id}')\n",
    "        mosaic_violins[row_id].append(f'Empty row {row_id}')\n",
    "        \n",
    "is_explain = False\n",
    "\n",
    "models_ids = [\n",
    "    145,\n",
    "    # 199,\n",
    "    # 600,\n",
    "    # 858,\n",
    "    # 966,\n",
    "    # 1692,\n",
    "    # 1724,\n",
    "    # 1725,\n",
    "    # 1950,\n",
    "    # 1952,\n",
    "]\n",
    "\n",
    "models_ids = sorted(list(set(models_ids)))\n",
    "\n",
    "n_models = 6\n",
    "fn_sweep = (\n",
    "    f\"models({n_models})_\"\n",
    "    f\"tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_\"\n",
    "    f\"val({val_random_state}_{val_n_splits}_{val_n_repeats})\"\n",
    ")\n",
    "df_sweeps = pd.read_excel(f\"{path}/pytorch_tabular/{fn_sweep}.xlsx\", index_col=0)\n",
    "path_models = f\"{path}/pytorch_tabular/candidates/{fn_sweep}\"\n",
    "pathlib.Path(path_models).mkdir(parents=True, exist_ok=True)\n",
    "df_sweeps.style.background_gradient(\n",
    "    subset=[\n",
    "        \"train_loss\",\n",
    "        \"validation_loss\",\n",
    "        \"test_loss\",\n",
    "        \"time_taken\",\n",
    "        \"time_taken_per_epoch\"\n",
    "    ], cmap=\"RdYlGn_r\"\n",
    ").to_excel(f\"{path_models}/sweep.xlsx\")\n",
    "df_sweeps.loc[models_ids, :].style.background_gradient(\n",
    "    subset=[\n",
    "        \"train_loss\",\n",
    "        \"validation_loss\",\n",
    "        \"test_loss\",\n",
    "        \"time_taken\",\n",
    "        \"time_taken_per_epoch\"\n",
    "    ], cmap=\"RdYlGn_r\"\n",
    ").to_excel(f\"{path_models}/selected.xlsx\")\n",
    "\n",
    "explain_method = \"GradientShap\"\n",
    "explain_baselines = \"b|1000\"\n",
    "explain_n_feats_to_plot = 25\n",
    "\n",
    "df_models_metrics = pd.DataFrame(index=models_ids)\n",
    "df_models_check = pd.DataFrame(index=models_ids)\n",
    "\n",
    "df_gses_models_metrics = {}\n",
    "for md in ['MAE All', 'MAE Test', 'Rho All', 'Rho Test', 'Bias All', 'Bias Test']:\n",
    "    df_gses_models_metrics[md] = pd.DataFrame(index=gses, columns=['Count'] + list(models_ids))\n",
    "    df_gses_models_metrics[md].loc[gses, 'Count'] = gse_count[gses]\n",
    "\n",
    "for model_id in models_ids:\n",
    "    print(model_id)\n",
    "\n",
    "    df_models_check.at[model_id, 'Train MAE Before'] = df_sweeps.at[model_id, 'train_mean_absolute_error']\n",
    "    df_models_check.at[model_id, 'Validation MAE Before'] = df_sweeps.at[model_id, 'validation_mean_absolute_error']\n",
    "    df_models_check.at[model_id, 'Test MAE Before'] = df_sweeps.at[model_id, 'test_mean_absolute_error']\n",
    "    df_models_check.at[model_id, 'Train Rho Before'] = df_sweeps.at[model_id, 'train_pearson_corrcoef']\n",
    "    df_models_check.at[model_id, 'Validation Rho Before'] = df_sweeps.at[model_id, 'validation_pearson_corrcoef']\n",
    "    df_models_check.at[model_id, 'Test Rho Before'] = df_sweeps.at[model_id, 'test_pearson_corrcoef']\n",
    "\n",
    "    split_id = df_sweeps.at[model_id, 'split_id']\n",
    "    fold_id = df_sweeps.at[model_id, 'fold_id']\n",
    "    split_dict = samples[split_id]\n",
    "\n",
    "    test = data.loc[split_dict['test'], feats + [\"Age\"]]\n",
    "    train = data.loc[split_dict['trains'][fold_id], feats + [\"Age\"]]\n",
    "    validation = data.loc[split_dict['validations'][fold_id], feats + [\"Age\"]]\n",
    "\n",
    "    model_dir = str(pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).parent).replace('\\\\', '/') + '/' + pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).stem\n",
    "    model = TabularModel.load_model(model_dir)\n",
    "    pathlib.Path(f\"{path_models}/{model_id}\").mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copytree(model_dir, f\"{path_models}/{model_id}\", dirs_exist_ok=True)\n",
    "\n",
    "    data_full['Prediction'] = model.predict(data_full)\n",
    "    data_full['Error'] = data_full['Prediction'] - data_full['Age']\n",
    "    data_full[['GPL', 'GSE', 'Age', 'Sex', 'Status', 'Prediction', 'Error']].to_excel(f\"{path_models}/{model_id}/df_full.xlsx\")\n",
    "\n",
    "    # Mosaic violins plot\n",
    "    sns.set_theme(style='ticks')\n",
    "    fig_height = 5 * len(mosaic_violins)\n",
    "    fig_width = 1.4 * max_mosaic_row\n",
    "    fig, axs = plt.subplot_mosaic(mosaic=mosaic_violins, figsize=(fig_width, fig_height), gridspec_kw={}, sharey=False, sharex=False)\n",
    "\n",
    "    for plot_id, plot_row in df_groups.iterrows():\n",
    "\n",
    "        plot_statuses = [statuses_rename[x] for x in ast.literal_eval(plot_row['Statuses'])]\n",
    "        plot_groups = [(statuses_rename[x[0]], statuses_rename[x[1]]) for x in ast.literal_eval(plot_row['Groups'])]\n",
    "\n",
    "        df_plot = data_full.loc[(data_full['GSE'] == plot_row['GSE']) & (data_full['Status'].isin(plot_statuses)), ['Status', 'Error']]\n",
    "        plot_status_count = df_plot['Status'].value_counts()\n",
    "        plot_statuses_rename = {}\n",
    "        for x in plot_statuses:\n",
    "            plot_statuses_rename[x] = x + f\"\\nCount: {plot_status_count[x]}\\nBias: {np.mean(df_plot.loc[df_plot['Status'] == x, 'Error']):0.1f}\"\n",
    "        plot_statuses = [plot_statuses_rename[x] for x in plot_statuses]\n",
    "        plot_groups = [(plot_statuses_rename[x[0]], plot_statuses_rename[x[1]]) for x in plot_groups]\n",
    "        df_plot['Status'] = df_plot['Status'].replace(plot_statuses_rename)\n",
    "        colors_plot_status = {plot_statuses_rename[x]: colors_status[x] for x in plot_statuses_rename}\n",
    "        \n",
    "        pval_formatted = []\n",
    "        for plot_group in plot_groups:\n",
    "            stat, pval = mannwhitneyu(\n",
    "                df_plot.loc[df_plot[\"Status\"] == plot_group[0], \"Error\"].values,\n",
    "                df_plot.loc[df_plot[\"Status\"] == plot_group[1], \"Error\"].values,\n",
    "                alternative=\"two-sided\",\n",
    "            )\n",
    "            pval_formatted.append(f\"{pval:.1e}\")\n",
    "            \n",
    "        violinplot = sns.violinplot(\n",
    "            data=df_plot,\n",
    "            x='Status',\n",
    "            y='Error',\n",
    "            palette=colors_plot_status,\n",
    "            scale='width',\n",
    "            order=plot_statuses,\n",
    "            saturation=0.75,\n",
    "            legend=False,\n",
    "            ax=axs[plot_id]\n",
    "        )\n",
    "        annotator = Annotator(\n",
    "            ax=axs[plot_id],\n",
    "            pairs=plot_groups,\n",
    "            data=df_plot,\n",
    "            x=\"Status\",\n",
    "            y=\"Error\",\n",
    "            order=plot_statuses,\n",
    "        )\n",
    "        annotator.set_custom_annotations(pval_formatted)\n",
    "        annotator.configure(loc='inside', verbose=0)\n",
    "        annotator.annotate()\n",
    "\n",
    "        axs[plot_id].set_xlabel('')\n",
    "        axs[plot_id].set_title(f\"{plot_row['GSE']} ({df_plot.shape[0]})\")\n",
    "        axs[plot_id].set_facecolor(colors_icd_chpts[plot_row['ICD-11 chapter']])\n",
    "\n",
    "    for empty_panel in violons_empty_panels:\n",
    "        axs[empty_panel].axis('off')\n",
    "    fig.tight_layout()    \n",
    "    fig.savefig(f\"{path_models}/{model_id}/violins_Status.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_models}/{model_id}/violins_Status.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    data['Group'] = ''\n",
    "    data.loc[train.index, 'Group'] = 'Train'\n",
    "    data.loc[validation.index, 'Group'] = 'Validation'\n",
    "    data.loc[test.index, 'Group'] = 'Test'\n",
    "    data['Prediction'] = model.predict(data)\n",
    "    data['Error'] = data['Prediction'] - data['Age']\n",
    "    data[['GPL', 'GSE', 'Age', 'Sex', 'Status', 'Group', 'Prediction', 'Error']].to_excel(f\"{path_models}/{model_id}/data_ctrl.xlsx\")\n",
    "\n",
    "    pred = torch.from_numpy(data.loc[:, 'Prediction'].values)\n",
    "    real = torch.from_numpy(data.loc[:, 'Age'].values)\n",
    "    df_models_metrics.at[model_id, f'MAE\\nAll'] = mean_absolute_error(pred, real).numpy()\n",
    "    df_models_metrics.at[model_id, f'Rho\\nAll'] = pearson_corrcoef(pred, real).numpy()\n",
    "    df_models_metrics.at[model_id, f'Bias\\nAll'] = np.mean(data.loc[:, 'Error'].values)\n",
    "\n",
    "    colors_groups = {\n",
    "        'Train': 'chartreuse',\n",
    "        'Validation': 'dodgerblue',\n",
    "        'Test': 'crimson',\n",
    "    }\n",
    "    df_metrics = pd.DataFrame(\n",
    "        index=list(colors_groups.keys()),\n",
    "        columns=['mean_absolute_error', 'pearson_corrcoef', 'bias']\n",
    "    )\n",
    "    for group in colors_groups.keys():\n",
    "\n",
    "        pred = torch.from_numpy(data.loc[data['Group'] == group, 'Prediction'].values)\n",
    "        real = torch.from_numpy(data.loc[data['Group'] == group, 'Age'].values)\n",
    "\n",
    "        df_metrics.at[group, 'mean_absolute_error'] = mean_absolute_error(pred, real).numpy()\n",
    "        df_metrics.at[group, 'pearson_corrcoef'] = pearson_corrcoef(pred, real).numpy()\n",
    "        df_metrics.at[group, 'bias'] = np.mean(data.loc[data['Group'] == group, 'Error'].values)\n",
    "\n",
    "        df_models_metrics.at[model_id, f'MAE\\n{group}'] = df_metrics.at[group, 'mean_absolute_error']\n",
    "        df_models_metrics.at[model_id, f'Rho\\n{group}'] = df_metrics.at[group, 'pearson_corrcoef']\n",
    "        df_models_metrics.at[model_id, f'Bias\\n{group}'] = df_metrics.at[group, 'bias']\n",
    "\n",
    "        df_models_check.at[model_id, f'{group} MAE After'] = df_metrics.at[group, 'mean_absolute_error']\n",
    "        df_models_check.at[model_id, f'{group} Rho After'] = df_metrics.at[group, 'pearson_corrcoef']\n",
    "\n",
    "    df_metrics.to_excel(f\"{path_models}/{model_id}/metrics.xlsx\", index_label=\"Metrics\")\n",
    "\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "    scatter = sns.scatterplot(\n",
    "        data=data,\n",
    "        x='Age',\n",
    "        y=\"Prediction\",\n",
    "        hue=\"Group\",\n",
    "        palette=colors_groups,\n",
    "        linewidth=0.2,\n",
    "        alpha=0.75,\n",
    "        edgecolor=\"k\",\n",
    "        s=20,\n",
    "        hue_order=list(colors_groups.keys()),\n",
    "        ax=ax\n",
    "    )\n",
    "    xy_min = data[['Age', 'Prediction']].min().min()\n",
    "    xy_max = data[['Age', 'Prediction']].max().max()\n",
    "    xy_ptp = xy_max - xy_min\n",
    "    bisect = sns.lineplot(\n",
    "        x=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        y=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "        linestyle='--',\n",
    "        color='black',\n",
    "        linewidth=1.0,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params, {df_sweeps.at[model_id, 'epochs']} epochs)\")\n",
    "    ax.set_xlim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    ax.set_ylim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    fig.savefig(f\"{path_models}/{model_id}/scatter.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_models}/{model_id}/scatter.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    for gse, ids in gse_ids.items():\n",
    "        pred = torch.from_numpy(data.loc[ids, 'Prediction'].values)\n",
    "        real = torch.from_numpy(data.loc[ids, 'Age'].values)\n",
    "        df_models_metrics.at[model_id, f'MAE\\n{gse}\\nAll'] = mean_absolute_error(pred, real).numpy()\n",
    "        df_models_metrics.at[model_id, f'Rho\\n{gse}\\nAll'] = pearson_corrcoef(pred, real).numpy()\n",
    "        df_models_metrics.at[model_id, f'Bias\\n{gse}\\nAll'] = np.mean(data.loc[ids, 'Error'].values)\n",
    "        df_gses_models_metrics['MAE All'].at[gse, model_id] = df_models_metrics.at[model_id, f'MAE\\n{gse}\\nAll']\n",
    "        df_gses_models_metrics['Rho All'].at[gse, model_id] = df_models_metrics.at[model_id, f'Rho\\n{gse}\\nAll']\n",
    "        df_gses_models_metrics['Bias All'].at[gse, model_id] = df_models_metrics.at[model_id, f'Bias\\n{gse}\\nAll']\n",
    "        for group in colors_groups.keys():\n",
    "            gse_group_ids = data.index[(data['Group'] == group) & (data['GSE'] == gse)].values\n",
    "            if len(gse_group_ids) > 0:\n",
    "                pred = torch.from_numpy(data.loc[gse_group_ids, 'Prediction'].values)\n",
    "                real = torch.from_numpy(data.loc[gse_group_ids, 'Age'].values)\n",
    "                df_models_metrics.at[model_id, f'MAE\\n{gse}\\n{group}'] = mean_absolute_error(pred, real).numpy()\n",
    "                df_models_metrics.at[model_id, f'Rho\\n{gse}\\n{group}'] = pearson_corrcoef(pred, real).numpy()\n",
    "                df_models_metrics.at[model_id, f'Bias\\n{gse}\\n{group}'] = np.mean(data.loc[gse_group_ids, 'Error'].values)\n",
    "                if group == 'Test':\n",
    "                    df_gses_models_metrics['MAE Test'].at[gse, model_id] = df_models_metrics.at[model_id, f'MAE\\n{gse}\\n{group}']\n",
    "                    df_gses_models_metrics['Rho Test'].at[gse, model_id] = df_models_metrics.at[model_id, f'Rho\\n{gse}\\n{group}']\n",
    "                    df_gses_models_metrics['Bias Test'].at[gse, model_id] = df_models_metrics.at[model_id, f'Bias\\n{gse}\\n{group}']\n",
    "\n",
    "    ids_parts = {\n",
    "        'All': data.index.values,\n",
    "        'Test': data.index[data['Group'] == 'Test'].values,\n",
    "    }\n",
    "    for part, ids_part in ids_parts.items():\n",
    "        # Errors in GSEs\n",
    "        df_fig = data.loc[ids_part, ['Error', 'GSE']].copy()\n",
    "        df_fig['GSE'] = pd.Categorical(df_fig.GSE, categories=gses, ordered=True)\n",
    "        df_fig = df_fig.sort_values('GSE')\n",
    "        gses_rename = {\n",
    "            gse: f\"{gse} ({gse_count[gse]})\" + \"\\n\" +\n",
    "                   fr\"MAE: {df_gses_models_metrics[f'MAE {part}'].at[gse, model_id]:0.2f}\" + \"\\n\"\n",
    "                   fr\"Pearson $\\rho$: {df_gses_models_metrics[f'Rho {part}'].at[gse, model_id]:0.2f}\" + \"\\n\" +\n",
    "                   fr\"Bias: {df_gses_models_metrics[f'Bias {part}'].at[gse, model_id]:0.2f}\"  + \"\\n\" + \n",
    "                   f\"{gse_preproc.at[gse, 'Preproc']}\"\n",
    "            for gse in colors_gse\n",
    "        }\n",
    "        gse_colors_grid = {gses_rename[gse]: colors_gse[gse] for gse in colors_gse}\n",
    "        df_fig['GSE'].replace(gses_rename, inplace=True) \n",
    "        sns.set_theme(style=\"whitegrid\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "        g = sns.FacetGrid(df_fig, row=\"GSE\", hue=\"GSE\", aspect=8, height=0.5, palette=gse_colors_grid)\n",
    "        g.map(\n",
    "            sns.kdeplot,\n",
    "            'Error',\n",
    "            fill=True, \n",
    "            alpha=1.0,\n",
    "            linewidth=0.5\n",
    "        )\n",
    "        g.map(\n",
    "            sns.kdeplot,\n",
    "            'Error',\n",
    "            color=\"black\",\n",
    "            linewidth=1.0,\n",
    "        )\n",
    "        # g.refline(y=0, linewidth=2.0, linestyle=\"-\", color=None, clip_on=False)\n",
    "        def label(x, color, label):\n",
    "            ax = plt.gca()\n",
    "            ax.text(-0.15, 0.2, label, size=4, fontweight=\"light\", color=color, ha=\"left\", va=\"center\", transform=ax.transAxes, path_effects=[pe.withStroke(linewidth=0.5, foreground=\"black\")])\n",
    "        g.map(label, 'Error')\n",
    "        # Set the subplots to overlap\n",
    "        g.figure.subplots_adjust(hspace=0.0)\n",
    "        # Remove axes details that don't play well with overlap\n",
    "        g.set_titles(\"\")\n",
    "        g.set(yticks=[], ylabel=\"\")\n",
    "        g.despine(bottom=True, left=True)\n",
    "        # Save\n",
    "        g.savefig(f\"{path_models}/{model_id}/GSEs_Error_{part}.png\", bbox_inches='tight', dpi=200)\n",
    "        g.savefig(f\"{path_models}/{model_id}/GSEs_Error_{part}.pdf\", bbox_inches='tight')\n",
    "        plt.close(g.fig)\n",
    "\n",
    "    # sns.set_theme(style='whitegrid')\n",
    "    # fig, ax = plt.subplots(figsize=(4.5, 4))\n",
    "    # kdeplot = sns.kdeplot(\n",
    "    #     data=data,\n",
    "    #     x='Age',\n",
    "    #     y='Prediction',\n",
    "    #     fill=True,\n",
    "    #     cbar=False,\n",
    "    #     color='paleturquoise',\n",
    "    #     cut=0,\n",
    "    #     legend=False,\n",
    "    #     ax=ax\n",
    "    # )\n",
    "    # bisect = sns.lineplot(\n",
    "    #     x=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "    #     y=[xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp],\n",
    "    #     linestyle='--',\n",
    "    #     color='black',\n",
    "    #     linewidth=1.0,\n",
    "    #     ax=ax\n",
    "    # )\n",
    "    # ax.set_title(f\"{df_sweeps.at[model_id, 'model']} ({df_sweeps.at[model_id, '# Params']} params, {df_sweeps.at[model_id, 'epochs']} epochs)\")\n",
    "    # ax.set_xlim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    # ax.set_ylim(xy_min - 0.1 * xy_ptp, xy_max + 0.1 * xy_ptp)\n",
    "    # plt.gca().set_aspect('equal', adjustable='box')\n",
    "    # fig.savefig(f\"{path_models}/{model_id}/kde.png\", bbox_inches='tight', dpi=200)\n",
    "    # fig.savefig(f\"{path_models}/{model_id}/kde.pdf\", bbox_inches='tight')\n",
    "    # plt.close(fig)\n",
    "\n",
    "    df_fig = data.loc[:, ['Error', 'Group']]\n",
    "    groups_rename = {\n",
    "        group: f\"{group}\" + \"\\n\" +\n",
    "               fr\"MAE: {df_metrics.at[group, 'mean_absolute_error']:0.2f}\" + \"\\n\"\n",
    "               fr\"Pearson $\\rho$: {df_metrics.at[group, 'pearson_corrcoef']:0.2f}\" + \"\\n\" +\n",
    "               fr\"$\\langle$Error$\\rangle$: {df_metrics.at[group, 'bias']:0.2f}\" \n",
    "        for group in colors_groups\n",
    "    }\n",
    "    colors_groups_violin = {groups_rename[group]: colors_groups[group] for group in colors_groups}\n",
    "    df_fig['Group'].replace(groups_rename, inplace=True)\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    violin = sns.violinplot(\n",
    "        data=df_fig,\n",
    "        x='Group',\n",
    "        y='Error',\n",
    "        palette=colors_groups_violin,\n",
    "        scale='width',\n",
    "        order=list(colors_groups_violin.keys()),\n",
    "        saturation=0.75,\n",
    "        legend=False,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_xlabel('')\n",
    "    fig.savefig(f\"{path_models}/{model_id}/violin.png\", bbox_inches='tight', dpi=200)\n",
    "    fig.savefig(f\"{path_models}/{model_id}/violin.pdf\", bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    if is_explain:\n",
    "        try:\n",
    "            explanation = model.explain(data, method=explain_method, baselines=explain_baselines)\n",
    "            explanation.index = data.index\n",
    "            explanation.to_excel(f\"{path_models}/{model_id}/explanation.xlsx\")\n",
    "\n",
    "            sns.set_theme(style='whitegrid')\n",
    "            fig = shap.summary_plot(\n",
    "                shap_values=explanation.loc[:, feats].values,\n",
    "                features=data.loc[:, feats].values,\n",
    "                feature_names=feats,\n",
    "                max_display=explain_n_feats_to_plot,\n",
    "                plot_type=\"violin\",\n",
    "                show=False,\n",
    "            )\n",
    "            plt.savefig(f\"{path_models}/{model_id}/explain_beeswarm.png\", bbox_inches='tight', dpi=200)\n",
    "            plt.savefig(f\"{path_models}/{model_id}/explain_beeswarm.pdf\", bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "            sns.set_theme(style='whitegrid')\n",
    "            fig = shap.summary_plot(\n",
    "                shap_values=explanation.loc[:, feats].values,\n",
    "                features=data.loc[:, feats].values,\n",
    "                feature_names=feats,\n",
    "                max_display=explain_n_feats_to_plot,\n",
    "                plot_type=\"bar\",\n",
    "                show=False,\n",
    "            )\n",
    "            plt.savefig(f\"{path_models}/{model_id}/explain_bar.png\", bbox_inches='tight', dpi=200)\n",
    "            plt.savefig(f\"{path_models}/{model_id}/explain_bar.pdf\", bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "        except NotImplementedError:\n",
    "            pass\n",
    "\n",
    "df_models_metrics.to_excel(f\"{path_models}/models_metrics.xlsx\", index_label='Model ID')\n",
    "\n",
    "for group in ['Train', 'Validation', 'Test']:\n",
    "    df_models_check[f'{group} MAE Diff'] = df_models_check[f'{group} MAE After'] - df_models_check[f'{group} MAE Before']\n",
    "    df_models_check[f'{group} Rho Diff'] = df_models_check[f'{group} Rho After'] - df_models_check[f'{group} Rho Before']\n",
    "df_models_check.to_excel(f\"{path_models}/models_check.xlsx\", index_label='Model ID')\n",
    "\n",
    "with pd.ExcelWriter(f\"{path_models}/gses_models_metrics.xlsx\", engine='xlsxwriter') as writer:\n",
    "    for md in ['MAE All', 'MAE Test', 'Rho All', 'Rho Test', 'Bias All', 'Bias Test']:\n",
    "        df_gses_models_metrics[md].insert(1, 'Preproc', gse_preproc.loc[df_gses_models_metrics[md].index, 'Preproc'])\n",
    "        df_gses_models_metrics[md].to_excel(writer, sheet_name=md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All models processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = 6\n",
    "fn_sweep = (\n",
    "    f\"models({n_models})_\"\n",
    "    f\"tst({tst_random_state}_{tst_n_splits}_{tst_n_repeats})_\"\n",
    "    f\"val({val_random_state}_{val_n_splits}_{val_n_repeats})\"\n",
    ")\n",
    "df_sweeps = pd.read_excel(f\"{path}/pytorch_tabular/selected.xlsx\", index_col=0)\n",
    "\n",
    "df_sweeps.insert(15, 'Passed\\nICD-11\\nTotal', None)\n",
    "ins_pos = 16\n",
    "for icd_chpt in icd_chpts:\n",
    "    df_sweeps.insert(ins_pos, f'Passed\\nICD-11\\nChapter {icd_chpt}', None)\n",
    "    ins_pos += 1\n",
    "\n",
    "for model_id, model_row in (pbar := tqdm(df_sweeps.iterrows())):\n",
    "    pbar.set_description(f\"Processing {model_id}\")\n",
    "\n",
    "    model_dir = str(pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).parent).replace('\\\\', '/') + '/' + pathlib.Path(df_sweeps.at[model_id, 'checkpoint']).stem\n",
    "    model = TabularModel.load_model(model_dir)\n",
    "\n",
    "    data_full['Prediction'] = model.predict(data_full)\n",
    "    data_full['Error'] = data_full['Prediction'] - data_full['Age']\n",
    "\n",
    "    passed_icd = {icd_chpt: 0 for icd_chpt in icd_chpts}\n",
    "    for icd_chpt in icd_chpts:\n",
    "        df_ckpt = df_groups[df_groups['ICD-11 chapter'] == icd_chpt]\n",
    "        for section_id, section_row in df_ckpt.iterrows():\n",
    "            section_statuses = ast.literal_eval(section_row['Statuses'])\n",
    "            section_groups = ast.literal_eval(section_row['Groups'])\n",
    "            section_directions = ast.literal_eval(section_row['Directions'])\n",
    "            df_section = data_full.loc[(data_full['GSE'] == section_row['GSE']) & (data_full['Status'].isin(section_statuses)), ['Status', 'Error']]\n",
    "            \n",
    "            for section_group_id, section_group in enumerate(section_groups):\n",
    "                stat, pval = mannwhitneyu(\n",
    "                    df_section.loc[df_section[\"Status\"] == section_group[0], \"Error\"].values,\n",
    "                    df_section.loc[df_section[\"Status\"] == section_group[1], \"Error\"].values,\n",
    "                    alternative=\"two-sided\",\n",
    "                )\n",
    "                bias_0 = np.mean(df_section.loc[df_section['Status'] == section_group[0], 'Error'])\n",
    "                bias_1 = np.mean(df_section.loc[df_section['Status'] == section_group[1], 'Error'])\n",
    "                group_direction = section_directions[section_group_id]\n",
    "                if pval < 0.05:\n",
    "                    if group_direction == 'Increasing' and bias_1 > bias_0:\n",
    "                        passed_icd[icd_chpt] += 1\n",
    "                    elif group_direction == 'Decreasing' and bias_1 < bias_0:\n",
    "                        passed_icd[icd_chpt] += 1\n",
    "        \n",
    "        df_sweeps.at[model_id, f'Passed\\nICD-11\\nChapter {icd_chpt}'] = passed_icd[icd_chpt]              \n",
    "        \n",
    "    df_sweeps.at[model_id, f'Passed\\nICD-11\\nTotal'] = sum(passed_icd.values())\n",
    "    \n",
    "df_sweeps.to_excel(f\"{path}/pytorch_tabular/models.xlsx\", index_label='Model ID')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
